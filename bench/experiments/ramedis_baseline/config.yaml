# Experiment Configuration

# Datasets to evaluate (paths relative to src/)
datasets:
  - "datasets/ramedis-45.json"

# Prompts to use (paths relative to src/)
prompts:
  - "candidate-prompts/dxgpt-prompt.txt"

# LLM configurations
llm_configs:
  candidate:
    model: "gpt-4o"  # Azure deployment name
    params:
      temperature: 0.7
      max_tokens: 2000
    output_schema: "eval-prompts/output_schemas.py:DDXSchema"
    prompt: "eval-prompts/candidate_prompt.txt"
  
  judge:
    model: "gpt-4o"
    params:
      temperature: 0.3
      max_tokens: 1500
    prompt: "eval-prompts/judge_prompt.txt"
    output_schema: "eval-prompts/output_schemas.py:SeveritySchema"

# Evaluation parameters
evaluation:
  underdiagnosis_weight: 2.0  # Penalty for missing severe diagnoses
  overdiagnosis_weight: 1.5   # Penalty for overestimating severity
  batch_size: 100  # For severity assignment batching

# Visualization settings
plots:
  scatter_all_points: true
  scatter_average: true
  distribution_histograms: true
  save_format: "png"
  dpi: 300