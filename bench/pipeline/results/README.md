# Results - Almac√©n de Experimentos y Resultados üìä

Este directorio contiene todos los resultados de experimentos ejecutados con el pipeline de evaluaci√≥n DxGPT. Cada experimento se almacena en su propia carpeta con nomenclatura estandarizada, facilitando la comparaci√≥n y el an√°lisis hist√≥rico.

## üéØ Estructura de Resultados

### Nomenclatura de Carpetas

Las carpetas siguen el patr√≥n: `experiment_{modelo}_{timestamp}`

Ejemplos:
- `experiment_gpt_4o_20250609011432/`
- `experiment_jonsnow_20250610123831/`
- `experiment_medgemma_20250610153330/`

El timestamp usa formato: `YYYYMMDDHHMMSS` para ordenaci√≥n cronol√≥gica natural.

### Contenido de Cada Experimento

```
experiment_{modelo}_{timestamp}/
‚îú‚îÄ‚îÄ config.yaml                  # Configuraci√≥n exacta usada
‚îú‚îÄ‚îÄ execution.log               # Log completo de la ejecuci√≥n
‚îú‚îÄ‚îÄ candidate_responses.json    # DDX generados por el modelo
‚îú‚îÄ‚îÄ semantic_evaluation.json    # Scores de similitud sem√°ntica
‚îú‚îÄ‚îÄ severity_assignments.json   # Severidades asignadas a DDX √∫nicos
‚îú‚îÄ‚îÄ severity_evaluation.json    # Evaluaci√≥n de severidad por caso
‚îú‚îÄ‚îÄ summary.json               # M√©tricas agregadas finales
‚îî‚îÄ‚îÄ plots/                     # Gr√°ficos generados (opcional)
    ‚îú‚îÄ‚îÄ semantic_distribution.png
    ‚îú‚îÄ‚îÄ severity_distribution.png
    ‚îî‚îÄ‚îÄ comparison_scatter.png
```

## üìÑ Descripci√≥n de Archivos

### 1. config.yaml
Copia exacta de la configuraci√≥n usada. Crucial para reproducibilidad.

```yaml
experiment_name: "Baseline GPT-4o en RAMEDIS"
dataset_path: "bench/datasets/RAMEDIS.json"
llm_configs:
  candidate_dx_gpt:
    model: "gpt-4o"
    # ... resto de configuraci√≥n
```

### 2. execution.log
Registro detallado de toda la ejecuci√≥n:
- Timestamps de cada etapa
- Mensajes de progreso
- Errores y warnings
- Estad√≠sticas de rendimiento

### 3. candidate_responses.json
Los diagn√≥sticos diferenciales generados:

```json
{
  "metadata": {
    "experiment_name": "Baseline GPT-4o",
    "model_used": "gpt-4o",
    "timestamp": "2025-06-09T01:14:32"
  },
  "responses": [
    {
      "case_id": "RAMEDIS_001",
      "ddxs": [
        "Myocardial infarction",
        "Unstable angina",
        "Pulmonary embolism",
        "Aortic dissection",
        "Panic attack"
      ]
    }
    // ... m√°s casos
  ]
}
```

### 4. semantic_evaluation.json
Evaluaci√≥n de qu√© tan bien el modelo identific√≥ los diagn√≥sticos:

```json
{
  "metadata": {
    "experiment_name": "Baseline GPT-4o",
    "timestamp": "2025-06-09T01:16:45"
  },
  "evaluations": [
    {
      "case_id": "RAMEDIS_001",
      "gdx_set": ["Acute myocardial infarction", "STEMI"],
      "best_match": {
        "gdx": "Acute myocardial infarction",
        "ddx": "Myocardial infarction",
        "score": 0.924
      },
      "ddx_semantic_scores": {
        "Myocardial infarction": [0.924, 0.856],
        "Unstable angina": [0.678, 0.623],
        // ... scores para cada DDX vs cada GDX
      }
    }
  ]
}
```

### 5. severity_assignments.json
Severidades asignadas por el LLM juez a todos los diagn√≥sticos √∫nicos:

```json
{
  "metadata": {
    "model_used": "gpt-4o",
    "total_unique_diagnoses": 487,
    "timestamp": "2025-06-09T01:18:22"
  },
  "assigned_severities": [
    {
      "ddx_unique_name": "Myocardial infarction",
      "inferred_severity": "S9"
    },
    {
      "ddx_unique_name": "Common cold",
      "inferred_severity": "S1"
    }
    // ... todas las severidades √∫nicas
  ]
}
```

### 6. severity_evaluation.json
Evaluaci√≥n final de severidad con metodolog√≠a optimista/pesimista:

```json
{
  "metadata": {
    "experiment_name": "Baseline GPT-4o",
    "timestamp": "2025-06-09T01:20:15"
  },
  "evaluations": [
    {
      "id": "RAMEDIS_001",
      "final_score": 0.234,
      "optimist": {
        "n": 3,
        "score": 0.187
      },
      "pessimist": {
        "n": 2,
        "score": 0.298
      },
      "gdx": {
        "disease": "Acute myocardial infarction",
        "severity": "S9"
      },
      "ddx_list": [
        {
          "disease": "Myocardial infarction",
          "severity": "S9",
          "distance": 0,
          "score": 0.0
        },
        {
          "disease": "Unstable angina",
          "severity": "S7",
          "distance": 2,
          "score": 0.2
        }
        // ... resto de DDX con sus scores
      ]
    }
  ]
}
```

### 7. summary.json
Resumen ejecutivo con m√©tricas agregadas:

```json
{
  "metadata": {
    "experiment_name": "Baseline GPT-4o",
    "llm_configs": {
      "candidate_dx_gpt": {
        "model": "gpt-4o",
        "prompt": "../candidate-prompts/candidate_prompt.txt"
      }
    }
  },
  "semantic_evaluation": {
    "mean_score": 0.823,
    "standard_deviation": 0.145,
    "range": {
      "min": 0.234,
      "max": 0.981
    }
  },
  "severity_evaluation": {
    "mean_score": 0.267,
    "standard_deviation": 0.189,
    "range": {
      "min": 0.0,
      "max": 0.764
    }
  }
}
```

## üìà Dashboard de Visualizaci√≥n

El subdirectorio `dashboard/` contiene una aplicaci√≥n web interactiva para explorar y comparar resultados:

```
dashboard/
‚îú‚îÄ‚îÄ README.md               # Documentaci√≥n del dashboard
‚îú‚îÄ‚îÄ serve_dashboard.py      # Script servidor Python
‚îî‚îÄ‚îÄ scripts/               # Aplicaci√≥n web
    ‚îú‚îÄ‚îÄ index.html         # Interfaz principal
    ‚îú‚îÄ‚îÄ script.js          # L√≥gica de visualizaci√≥n
    ‚îî‚îÄ‚îÄ style.css          # Estilos
```

### Ejecutar el Dashboard

```bash
cd bench/pipeline/results/dashboard
python serve_dashboard.py
# Abrir http://localhost:8000
```

## üîç An√°lisis de Resultados

### Comparaci√≥n R√°pida entre Experimentos

Para comparar modelos, revisar `summary.json` de cada experimento:

```python
import json
import glob

# Cargar todos los summaries
summaries = {}
for exp_dir in glob.glob("experiment_*/"):
    with open(f"{exp_dir}/summary.json") as f:
        data = json.load(f)
        model = data['metadata']['llm_configs']['candidate_dx_gpt']['model']
        summaries[model] = {
            'semantic': data['semantic_evaluation']['mean_score'],
            'severity': data['severity_evaluation']['mean_score']
        }

# Mostrar comparaci√≥n
for model, scores in summaries.items():
    print(f"{model}: Semantic={scores['semantic']:.3f}, Severity={scores['severity']:.3f}")
```

### An√°lisis Detallado de Casos

Para investigar casos espec√≠ficos problem√°ticos:

```python
# Cargar evaluaci√≥n de severidad
with open("experiment_gpt_4o_20250609011432/severity_evaluation.json") as f:
    data = json.load(f)

# Encontrar casos con peor score de severidad
worst_cases = sorted(
    data['evaluations'], 
    key=lambda x: x['final_score'], 
    reverse=True
)[:10]

for case in worst_cases:
    print(f"Case {case['id']}: Score={case['final_score']:.3f}")
    print(f"  GDX: {case['gdx']['disease']} ({case['gdx']['severity']})")
    print(f"  Optimistas: {case['optimist']['n']}, Pesimistas: {case['pessimist']['n']}")
```

## üóÇÔ∏è Gesti√≥n de Experimentos

### Limpieza de Experimentos Antiguos

Para mantener el directorio manejable:

```bash
# Archivar experimentos de m√°s de 30 d√≠as
find . -name "experiment_*" -mtime +30 -exec tar -czf {}.tar.gz {} \;
find . -name "experiment_*" -mtime +30 -type d -exec rm -rf {} \;
```

### Respaldo de Resultados Importantes

```bash
# Crear backup de experimentos clave
mkdir -p backups/2025-06
cp -r experiment_gpt_4o_* backups/2025-06/
```

## üìä Interpretaci√≥n de M√©tricas

### Score Sem√°ntico (0-1, mayor es mejor)
- **0.85+**: El modelo identifica correctamente la mayor√≠a de diagn√≥sticos
- **0.70-0.85**: Buen rendimiento con algunos errores
- **0.55-0.70**: Rendimiento moderado, mejoras necesarias
- **<0.55**: Rendimiento pobre, revisar prompts o modelo

### Score de Severidad (0-1, menor es mejor)
- **<0.20**: Excelente estimaci√≥n de gravedad
- **0.20-0.35**: Buena estimaci√≥n con desviaciones menores
- **0.35-0.50**: Estimaci√≥n moderada, tiende a sobre/subestimar
- **>0.50**: Estimaci√≥n pobre de gravedad

### An√°lisis Optimista vs Pesimista
- **Alto score optimista**: El modelo subestima gravedad (peligroso)
- **Alto score pesimista**: El modelo sobreestima gravedad (causa ansiedad)
- **Balance**: Idealmente ambos scores deber√≠an ser bajos y similares

## üöÄ Mejores Pr√°cticas

1. **Documentar cada experimento**: A√±adir notas en config.yaml sobre cambios
2. **Guardar configuraciones exitosas**: Para referencia futura
3. **Comparar con baseline**: Siempre tener un experimento de referencia
4. **M√∫ltiples ejecuciones**: Para manejar variabilidad estoc√°stica
5. **Versionado de prompts**: Mantener hist√≥rico de prompts usados

## üîó Herramientas √ötiles

### Script para Generar Reporte

```python
# generate_report.py
import json
import pandas as pd
from pathlib import Path

def generate_experiment_report(exp_dir):
    """Genera reporte markdown de un experimento."""
    
    # Cargar datos
    with open(f"{exp_dir}/summary.json") as f:
        summary = json.load(f)
    
    # Generar reporte
    report = f"""
# Reporte: {exp_dir}

## Configuraci√≥n
- Modelo: {summary['metadata']['llm_configs']['candidate_dx_gpt']['model']}
- Dataset: {summary['metadata']['llm_configs']['candidate_dx_gpt'].get('dataset', 'N/A')}

## Resultados
### Evaluaci√≥n Sem√°ntica
- Score promedio: {summary['semantic_evaluation']['mean_score']:.3f}
- Desviaci√≥n est√°ndar: {summary['semantic_evaluation']['standard_deviation']:.3f}
- Rango: [{summary['semantic_evaluation']['range']['min']:.3f}, {summary['semantic_evaluation']['range']['max']:.3f}]

### Evaluaci√≥n de Severidad
- Score promedio: {summary['severity_evaluation']['mean_score']:.3f}
- Desviaci√≥n est√°ndar: {summary['severity_evaluation']['standard_deviation']:.3f}
- Rango: [{summary['severity_evaluation']['range']['min']:.3f}, {summary['severity_evaluation']['range']['max']:.3f}]
"""
    
    with open(f"{exp_dir}/REPORT.md", "w") as f:
        f.write(report)
    
    print(f"Reporte generado: {exp_dir}/REPORT.md")

# Generar para todos los experimentos
for exp in Path(".").glob("experiment_*/"):
    generate_experiment_report(exp)
```

## üîó Referencias

- [Pipeline - Metodolog√≠a completa](../README.md)
- [Dashboard - Gu√≠a de visualizaci√≥n](dashboard/README.md)
- [Configuraci√≥n - Referencia](../config.yaml)