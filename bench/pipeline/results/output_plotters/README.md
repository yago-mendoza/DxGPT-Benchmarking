# Visualising Experiment Output

When running an experiment using `run.py`, multiple output files are generated in JSON format. Each serves a specific role in **evaluation** and **comparison**:

- **`semantic_evaluation.json`**  
  Contains the semantic accuracy scores for each diagnosis generated in the experiment. It evaluates how closely each DDX (differential diagnosis) matches the corresponding GDX (ground truth diagnosis) based on semantic similarity.

- **`severity_evaluation.json`**  
  Reports the severity score for each case, quantifying how much the predicted severity levels deviate from the ground truth. This helps assess the clinical relevance of the predictions. Despite its name, the "severity score" is actually a distance from the ground truth.

Used to visualize **experiment performance** via `/results/plotting/compare_runs.html`  

- **`summary.json` (multiple files)**  
  Each experiment produces a `summary.json` file, which aggregates key metrics and metadata. These summaries are essential for comparing different experimental runs.
 
Used to **compare different experiments** via `/results/plotting/visualize_run_evals.html`

### Key Concepts

- **Case**: Each evaluation unit, typically associated with one primary GDX
- **GDX**: Ground truth diagnosis (1 per case)
- **DDX**: Differential diagnoses (5 per case)
- **Semantic Score**: Maximum similarity score across all GDX-DDX combinations for a case
- **Severity Score**: Normalized average distance between GDX and DDX severities

## ðŸ“Š Evaluating a single run (intra-run)

The analysis requires two JSON files, generated by running `run.py`:

1. **Semantic Evaluation** (`*semantic_evaluation*.json`): Contains BERT-based similarity scores [0,1] between ground truth diagnoses (GDX) and differential diagnoses (DDX).
2. **Severity Evaluation** (`*severity_evaluation*.json`): Contains severity assessments (S0-S10 scale) assigned by an LLM for both GDX and DDX.

### 1. Both

### ðŸ“‰ Combined Bias Evaluation Plot

This scatter plot reveals the model's diagnostic tendencies:
- **Y-axis**: Semantic similarity score [0,1]
- **X-axis**: Severity score (distance), positioned left (negative) or right (positive) based on bias
- **Positioning logic**:
  - Calculate aggregate distances for DDX below GDX (optimistic) and above GDX (pessimistic)
  - If optimistic distances > pessimistic distances â†’ place left (blue)
  - If pessimistic distances > optimistic distances â†’ place right (red)
  - Equal distances â†’ center (gray)
- **Bottom histogram**: Distribution of severity scores across all cases
- **Mean markers**: Show average positions for each group and overall

**Interpretation**: Left-side clustering indicates the model tends to underestimate severity (optimistic bias), while right-side clustering indicates overestimation (pessimistic bias).

### 2. Semantic Analysis Plots

### ðŸ“‰ Semantic Score Distribution
Simple histogram with KDE overlay showing the distribution of semantic similarity scores across all cases. Higher scores indicate better diagnostic accuracy.

### ðŸ“‰ Ridge Plot
Overlapping density distributions of semantic scores, stratified by GDX severity level (S3-S10). This reveals whether diagnostic accuracy varies with disease severity. Each ridge represents the distribution of best-match semantic scores for cases with that GDX severity.

### 3. Severity Analysis Plots

### ðŸ“‰ GDX vs DDX Severity Comparison
Scatter plot with boxplots showing the relationship between ground truth and predicted severities:
- **X-axis**: GDX severity (S0-S10)
- **Y-axis**: Mean severity of the 5 DDX
- **Diagonal line**: Perfect prediction (y=x)
- **Blue points/boxplots**: Cases where DDX severities < GDX (optimistic)
- **Red points/boxplots**: Cases where DDX severities > GDX (pessimistic)
- **Bottom histogram**: Distribution of GDX severities

**Interpretation**: Points below the diagonal indicate underestimation of severity; points above indicate overestimation.

### ðŸ“‰ Severity Levels Distribution
Comparative histogram showing the frequency of each severity level (S3-S10):
- **Green bars**: GDX counts (Ã—5 for visual scaling to match DDX volume)
- **Orange bars**: DDX counts
- **KDE curves**: Smooth distribution overlays

This visualization reveals whether the model's severity predictions follow the same distribution as the ground truth.

### ðŸ“‰ Optimist vs Pessimist Evaluator Balance
Doughnut chart showing the distribution of optimistic vs pessimistic cases:
- **Blue segments**: Optimistic cases (DDX severities tend to be lower than GDX)
- **Red segments**: Pessimistic cases (DDX severities tend to be higher than GDX)
- **Gradient shading**: Darker shades represent higher values of n (1-5), where n indicates how many of the 5 DDX fall into that category
- **Center text**: Total counts for each bias type

**Interpretation**: A balanced model would show roughly equal blue and red areas. Dominance of blue indicates systematic underestimation of severity, while red dominance indicates overestimation.

## ðŸ›’ Comparing experiment configurations (inter-run)

### Summary generations (@`run.py`)

To compare different experiment results, use the `compare_runs.html` tool. It leverages the `summary.json` files to provide a visual and quantitative comparison between multiple models or configurations.

The `summary.json` files are automatically generated by the `run.py` script after each experiment run. These files contain consolidated metrics for easy comparison between different model configurations. The summary structure includes:

```json
{
    "metadata": {
        "experiment_name": "Model Description",
        "llm_configs": {
            "candidate_dx_gpt": {
                "model": "model-name",
                "prompt": "prompt-file-path"
            },
            "severity_assigner_llm": {
                "model": "model-name", 
                "prompt": "prompt-file-path"
            }
        }
    },
    "semantic_evaluation": {
        "mean_score": 0.8582,
        "standard_deviation": 0.1989,
        "range": {
            "min": 0.3755,
            "max": 1.0000
        }
    },
    "severity_evaluation": {
        "mean_score": 0.1670,
        "standard_deviation": 0.0779,
        "range": {
            "min": 0.0500,
            "max": 0.4000
        }
    }
}

```

### ðŸ“‰ Semantics vs Severity Plot 

The `compare_runs` functionality allows you to load multiple `summary.json` files from different experiment runs to generate comparative visualizations. This creates a two-axis scatter plot showing:

- **X-axis**: Semantic evaluation mean scores (higher = better diagnostic accuracy)
- **Y-axis**: Severity evaluation mean scores (lower = better severity prediction)
- **Data points**: Each model/experiment represented as a point
- **Model labels**: Experiment names displayed for easy identification
- **Performance quadrants**: Visual regions showing different performance profiles

**Interpretation**: The ideal position is top-right (high semantic score, low severity score (a.k.a. distance)), indicating both accurate diagnoses and precise severity assessment. This comparative view enables quick identification of the best-performing model configurations and reveals trade-offs between semantic accuracy and severity prediction capabilities.