# Pipeline de Evaluaci√≥n DxGPT - Metodolog√≠a Detallada üî¨

Este directorio contiene el motor de evaluaci√≥n que implementa la metodolog√≠a cient√≠fica para medir el rendimiento de modelos de IA en diagn√≥stico m√©dico. El pipeline est√° dise√±ado para ser reproducible, extensible y estad√≠sticamente riguroso.

## üéØ Visi√≥n General

El pipeline eval√∫a modelos de diagn√≥stico mediante un proceso de 5 etapas que replica c√≥mo un m√©dico abordar√≠a un caso cl√≠nico:

1. **Generaci√≥n de Diagn√≥sticos Diferenciales (DDX)**
2. **Evaluaci√≥n Sem√°ntica** (¬øAcert√≥ el diagn√≥stico?)
3. **Asignaci√≥n de Severidades** (¬øQu√© tan graves son?)
4. **Evaluaci√≥n de Severidad** (¬øEstim√≥ bien la gravedad?)
5. **Agregaci√≥n de Resultados** (M√©tricas finales)

## üìä Metodolog√≠a de Evaluaci√≥n

### 1. Score Sem√°ntico (SapBERT como Juez)

El score sem√°ntico mide qu√© tan cerca est√° el modelo de identificar el diagn√≥stico correcto, usando embeddings m√©dicos especializados.

**Principio clave**: El objetivo es que el diagn√≥stico correcto est√© presente, sin importar su posici√≥n en la lista.

**Proceso**:
```python
# Para cada caso
DDX = ["Neumon√≠a", "Bronquitis", "COVID-19", "Gripe", "Asma"]
GDX = ["Neumon√≠a bacteriana", "Neumon√≠a viral"]

# SapBERT calcula similitud entre cada par DDX-GDX
similitudes = {
    "Neumon√≠a": {
        "Neumon√≠a bacteriana": 0.92,
        "Neumon√≠a viral": 0.89
    },
    "COVID-19": {
        "Neumon√≠a bacteriana": 0.45,
        "Neumon√≠a viral": 0.67
    }
    # ...
}

# Score final = mejor match encontrado
best_match = 0.92  # Neumon√≠a ‚Üî Neumon√≠a bacteriana
```

**Interpretaci√≥n**:
- En casos con comorbilidad, basta con acertar UNA de las condiciones
- Priorizamos cobertura sobre ranking exacto
- Un score > 0.8 indica identificaci√≥n correcta del diagn√≥stico

### 2. Score de Severidad (LLM como Juez)

El score de severidad eval√∫a si el modelo comprende la gravedad real de las condiciones diagnosticadas.

**Principio clave**: Consideramos TODOS los diagn√≥sticos generados porque aparecen simult√°neamente al paciente.

**Metodolog√≠a Nueva (Optimista vs Pesimista)**:

```python
# Severidades van de S0 (m√≠nima) a S10 (m√°xima)
GDX_severity = "S8"  # Diagn√≥stico real es grave
DDX_severities = ["S9", "S7", "S6", "S3", "S2"]  # Lo que predijo el modelo

# Calcular distancia m√°xima posible
if S_GDX <= 5:
    max_distance = 10 - S_GDX  # Puede sobrestimar hasta S10
else:
    max_distance = S_GDX  # Puede subestimar hasta S0

# Para cada DDX
for ddx_severity in DDX_severities:
    distance = abs(S_GDX - S_DDX)
    normalized = distance / max_distance
    
    # Categorizar
    if S_DDX < S_GDX:
        optimista.append(normalized)  # Subestim√≥ gravedad
    elif S_DDX > S_GDX:
        pesimista.append(normalized)  # Sobreestim√≥ gravedad
```

**M√©tricas resultantes**:
- **Score general**: Promedio de distancias normalizadas [0-1]
- **Score optimista**: Promedio cuando subestima gravedad
- **Score pesimista**: Promedio cuando sobreestima gravedad
- **0 = perfecto**, 1 = m√°ximo error posible

**¬øPor qu√© esta metodolog√≠a?**

1. **Distancia normalizada**: Penaliza proporcionalmente seg√∫n qu√© tan lejos est√° la estimaci√≥n
2. **Asimetr√≠a considerada**: Un S2‚ÜíS8 es diferente a S8‚ÜíS2 en implicaciones cl√≠nicas
3. **An√°lisis optimista/pesimista**: Revela si el modelo tiende a minimizar o exagerar

## üîß Implementaci√≥n T√©cnica

### Archivo Principal: `run.py`

El script orquesta todo el pipeline con estas funciones clave:

#### Etapa 1: Generaci√≥n de DDX
```python
def generate_candidate_diagnoses(cases, llm, prompt_template, schema, logger):
    """
    Genera 5 diagn√≥sticos diferenciales por caso.
    
    - Usa el modelo configurado (GPT-4, MedGemma, etc.)
    - Aplica el prompt de candidate_prompt.txt
    - Fuerza exactamente 5 DDX por caso
    """
```

#### Etapa 2: Evaluaci√≥n Sem√°ntica
```python
def process_semantic_evaluation_parallel(cases, candidate_responses, logger):
    """
    Eval√∫a similitud DDX vs GDX en paralelo.
    
    - Warm-up del endpoint SapBERT primero
    - Procesamiento batch para eficiencia
    - ThreadPoolExecutor para paralelismo
    - Retorna best_match por caso
    """
```

#### Etapa 3: Asignaci√≥n de Severidades
```python
def assign_severities_batch(llm, prompt_template, unique_ddxs, schema, logger):
    """
    Asigna severidad S0-S10 a diagn√≥sticos √∫nicos.
    
    - Procesamiento en batches de 50
    - Un solo LLM call por batch
    - Manejo robusto de formatos de respuesta
    """
```

#### Etapa 4: Evaluaci√≥n de Severidad
```python
def evaluate_case_severity(case_id, ddxs, gdxs, best_gdx_name, 
                          severity_assignments, logger):
    """
    Calcula scores de severidad con metodolog√≠a optimista/pesimista.
    
    - Usa el mejor GDX del match sem√°ntico
    - Normaliza por distancia m√°xima posible
    - Separa en categor√≠as optimista/pesimista
    - Retorna estructura detallada con todas las m√©tricas
    """
```

### Configuraci√≥n: `config.yaml`

```yaml
experiment_name: "Nombre descriptivo del experimento"
dataset_path: "bench/datasets/RAMEDIS.json"

llm_configs:
  # Modelo que genera diagn√≥sticos
  candidate_dx_gpt:
    model: "gpt-4o"  # o "jonsnow", "medgemma", etc.
    prompt: "../candidate-prompts/candidate_prompt.txt"
    output_schema: "../candidate-prompts/candidate_output_schema.json"
    params:
      temperature: 0.7
      max_tokens: 500

  # Modelo que asigna severidades
  severity_assigner_llm:
    model: "gpt-4o"
    prompt: "eval-prompts/severity_assignment_batch_prompt.txt"
    output_schema: "eval-prompts/severity_assignment_batch_schema.json"
    params:
      temperature: 0.1  # Baja para consistencia
      max_tokens: 2000
```

## üìà Outputs y Resultados

### Estructura de Salida

Cada experimento genera:

```
results/experiment_{modelo}_{timestamp}/
‚îú‚îÄ‚îÄ config.yaml                  # Configuraci√≥n usada
‚îú‚îÄ‚îÄ execution.log               # Log detallado de ejecuci√≥n
‚îú‚îÄ‚îÄ candidate_responses.json    # DDX generados
‚îú‚îÄ‚îÄ semantic_evaluation.json    # Scores sem√°nticos
‚îú‚îÄ‚îÄ severity_assignments.json   # Severidades asignadas
‚îú‚îÄ‚îÄ severity_evaluation.json    # Evaluaci√≥n final
‚îú‚îÄ‚îÄ summary.json               # M√©tricas agregadas
‚îî‚îÄ‚îÄ plots/                     # Visualizaciones (si se generan)
```

### Formato de Resultados

#### severity_evaluation.json
```json
{
  "evaluations": [
    {
      "id": "CASE_001",
      "final_score": 0.234,
      "optimist": {
        "n": 3,
        "score": 0.187
      },
      "pessimist": {
        "n": 2,
        "score": 0.298
      },
      "gdx": {
        "disease": "Myocardial infarction",
        "severity": "S9"
      },
      "ddx_list": [
        {
          "disease": "Heart attack",
          "severity": "S9",
          "distance": 0,
          "score": 0.0
        },
        // ... m√°s DDX
      ]
    }
  ]
}
```

## üöÄ Ejecutar Experimentos

### Experimento B√°sico

```bash
cd bench/pipeline
python run.py
```

### Experimento con Dataset Peque√±o

```yaml
# En config.yaml
dataset_path: "bench/datasets/ramedis-5.json"
```

### Comparar M√∫ltiples Modelos

```bash
# Ejecutar con diferentes modelos
for model in gpt-4o jonsnow medgemma; do
    # Modificar config.yaml program√°ticamente
    sed -i "s/model: .*/model: $model/" config.yaml
    python run.py
done
```

## üìä Interpretaci√≥n de Resultados

### M√©tricas Clave en summary.json

```json
{
  "semantic_evaluation": {
    "mean_score": 0.823,        // Promedio de best matches
    "standard_deviation": 0.145,
    "range": {
      "min": 0.234,
      "max": 0.981
    }
  },
  "severity_evaluation": {
    "mean_score": 0.267,        // Promedio de errores de severidad
    "standard_deviation": 0.189,
    "range": {
      "min": 0.0,
      "max": 0.764
    }
  }
}
```

### Interpretaci√≥n:

**Score Sem√°ntico** (0-1, mayor es mejor):
- > 0.85: Excelente capacidad diagn√≥stica
- 0.70-0.85: Buena capacidad
- 0.55-0.70: Capacidad moderada
- < 0.55: Capacidad pobre

**Score de Severidad** (0-1, menor es mejor):
- < 0.20: Excelente estimaci√≥n de gravedad
- 0.20-0.35: Buena estimaci√≥n
- 0.35-0.50: Estimaci√≥n moderada
- > 0.50: Estimaci√≥n pobre

## üß™ Validaci√≥n y Testing

### Tests de Integridad

El pipeline incluye validaciones autom√°ticas:
- Exactamente 5 DDX por caso
- Severidades en rango S0-S10
- Scores en rango [0,1]
- JSONs bien formados

### Modo Debug

```python
# En run.py, cambiar nivel de logging
logging.basicConfig(level=logging.DEBUG)
```

### Validaci√≥n Manual

Revisar casos espec√≠ficos en los JSONs de salida para verificar:
- Diagn√≥sticos tienen sentido m√©dico
- Severidades son razonables
- Best matches son correctos

## üéØ Mejores Pr√°cticas

1. **Consistencia**: Usar mismo dataset y prompts para comparaciones justas
2. **Repetibilidad**: Fijar seeds y temperatura para reproducibilidad
3. **Documentaci√≥n**: Anotar cambios y observaciones en config.yaml
4. **Versionado**: Guardar configs exitosas para referencia
5. **Validaci√≥n**: Revisar manualmente subset de resultados

## üö® Limitaciones Conocidas

1. **Sesgo del juez**: LLM asignando severidades puede tener sus propios sesgos
2. **Cobertura ICD-10**: No todos los diagn√≥sticos tienen c√≥digo ICD-10
3. **Variabilidad**: Resultados pueden variar entre ejecuciones
4. **Escalabilidad**: Datasets muy grandes pueden ser costosos

## üîó Referencias

- [Documentaci√≥n de SapBERT](https://github.com/cambridgeltl/sapbert)
- [Escala de Severidad S0-S10](../docs/severity_scale.md)
- [Dashboard de Visualizaci√≥n](results/dashboard/README.md)