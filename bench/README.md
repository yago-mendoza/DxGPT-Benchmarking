# Bench - Sistema de EvaluaciÃ³n de Modelos de DiagnÃ³stico ğŸ†

El directorio `bench` contiene todo el sistema de benchmarking para evaluar modelos de IA mÃ©dica. Este es el nÃºcleo del proyecto DxGPT Latitude Bench, donde se ejecutan experimentos controlados para medir el rendimiento de diferentes modelos en tareas de diagnÃ³stico diferencial.

## ğŸ¯ PropÃ³sito

El sistema de bench estÃ¡ diseÃ±ado para responder una pregunta fundamental: **Â¿QuÃ© tan bien diagnostican los modelos de IA comparados con mÃ©dicos expertos?**

Para responder esto, evaluamos dos dimensiones crÃ­ticas:

1. **PrecisiÃ³n SemÃ¡ntica**: Â¿El modelo identificÃ³ el diagnÃ³stico correcto?
2. **EvaluaciÃ³n de Severidad**: Â¿El modelo estimÃ³ correctamente la gravedad?

## ğŸ—ï¸ Arquitectura del Sistema

```
bench/
â”œâ”€â”€ README.md                # Este archivo
â”œâ”€â”€ candidate-prompts/       # Prompts para generar diagnÃ³sticos
â”‚   â”œâ”€â”€ candidate_prompt.txt         # Prompt principal para DDX
â”‚   â””â”€â”€ candidate_output_schema.json # Esquema de respuesta esperada
â”œâ”€â”€ datasets/               # Datasets mÃ©dicos procesados
â”‚   â”œâ”€â”€ RAMEDIS.json       # Dataset completo (1000+ casos)
â”‚   â”œâ”€â”€ ramedis-45.json    # Subset mediano para pruebas
â”‚   â””â”€â”€ ramedis-5.json     # Mini subset para desarrollo
â””â”€â”€ pipeline/              # Motor de evaluaciÃ³n
    â”œâ”€â”€ run.py            # Script principal de ejecuciÃ³n
    â”œâ”€â”€ config.yaml       # ConfiguraciÃ³n de experimentos
    â”œâ”€â”€ eval-prompts/     # Prompts para evaluaciÃ³n
    â””â”€â”€ results/          # Resultados de experimentos
```

## ğŸ”„ Flujo de EvaluaciÃ³n

El proceso completo sigue estos pasos:

### 1. GeneraciÃ³n de DiagnÃ³sticos (DDX)
```python
# El modelo recibe un caso clÃ­nico
caso = "Paciente de 45 aÃ±os con dolor torÃ¡cico..."

# Genera 5 diagnÃ³sticos diferenciales
ddx = ["Infarto", "Angina", "Reflujo", "Ansiedad", "Costocondritis"]
```

### 2. EvaluaciÃ³n SemÃ¡ntica
```python
# Comparamos DDX con diagnÃ³sticos correctos (GDX)
gdx = ["Infarto agudo de miocardio", "SÃ­ndrome coronario agudo"]

# SapBERT calcula similitud semÃ¡ntica
scores = {
    "Infarto": {"Infarto agudo de miocardio": 0.95},
    "Angina": {"SÃ­ndrome coronario agudo": 0.78},
    # ...
}
```

### 3. AsignaciÃ³n de Severidad
```python
# Un LLM asigna severidad a cada diagnÃ³stico Ãºnico
severidades = {
    "Infarto": "S9",        # Muy grave
    "Angina": "S7",         # Grave
    "Reflujo": "S3",        # Leve
    "Ansiedad": "S2",       # Muy leve
    "Costocondritis": "S1"  # MÃ­nima
}
```

### 4. CÃ¡lculo de MÃ©tricas
```python
# Score semÃ¡ntico: mejor match entre DDX y GDX
semantic_score = 0.95  # (Infarto â†” Infarto agudo)

# Score de severidad: distancia normalizada
severity_score = 0.15  # (cercano a severidad correcta)
```

## ğŸ“ Componentes Principales

### candidate-prompts/
Contiene los prompts e instrucciones para que los modelos generen diagnÃ³sticos:

- **candidate_prompt.txt**: Prompt cuidadosamente diseÃ±ado que simula cÃ³mo un mÃ©dico abordarÃ­a un caso
- **candidate_output_schema.json**: Define el formato JSON esperado para las respuestas

### datasets/
Datasets mÃ©dicos validados y estructurados:

- **Origen**: Procesados desde `data29/` mediante ETL
- **Formato**: JSON con casos clÃ­nicos y diagnÃ³sticos de referencia
- **TamaÃ±os**: Desde 5 casos (desarrollo) hasta 1000+ (evaluaciÃ³n completa)

### pipeline/
El corazÃ³n del sistema de evaluaciÃ³n:

- **run.py**: Orquesta todo el proceso de evaluaciÃ³n
- **config.yaml**: Define quÃ© modelos evaluar y cÃ³mo
- **eval-prompts/**: Prompts para el juez de severidad
- **results/**: Almacena resultados de cada experimento

## ğŸš€ Ejecutar un Experimento

### 1. Configurar el experimento

Editar `pipeline/config.yaml`:
```yaml
experiment_name: "Mi Experimento GPT-4"
dataset_path: "bench/datasets/ramedis-45.json"

llm_configs:
  candidate_dx_gpt:
    model: "gpt-4o"  # o "jonsnow", "medgemma", etc.
    prompt: "../candidate-prompts/candidate_prompt.txt"
    
  severity_assigner_llm:
    model: "gpt-4o"
    prompt: "eval-prompts/severity_assignment_batch_prompt.txt"
```

### 2. Ejecutar

```bash
cd bench/pipeline
python run.py
```

### 3. Resultados

Se genera una carpeta en `results/` con:
- **candidate_responses.json**: DDX generados
- **semantic_evaluation.json**: Scores de similitud
- **severity_assignments.json**: Severidades asignadas
- **severity_evaluation.json**: EvaluaciÃ³n final
- **summary.json**: MÃ©tricas agregadas

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

### Score SemÃ¡ntico (0-1)
- **1.0**: Match perfecto con diagnÃ³stico correcto
- **0.8+**: Muy buena precisiÃ³n diagnÃ³stica
- **0.6+**: PrecisiÃ³n aceptable
- **<0.6**: PrecisiÃ³n pobre

### Score de Severidad (0-1)
- **0.0**: Severidad perfectamente estimada
- **<0.2**: Muy buena estimaciÃ³n
- **<0.4**: EstimaciÃ³n aceptable
- **>0.4**: EstimaciÃ³n pobre

## ğŸ§ª Modelos Evaluables

El sistema puede evaluar cualquier modelo que implemente la interfaz LLM:

### Azure OpenAI
- GPT-4o
- GPT-4-turbo
- GPT-3.5-turbo

### Hugging Face
- JonSnow (mÃ©dico especializado)
- MedGemma (Google)
- Sakura (multilingÃ¼e mÃ©dico)
- OpenBio (biomÃ©dico)

### AÃ±adir Nuevo Modelo

1. Configurar endpoint en `.env`:
   ```env
   MIMODELO_ENDPOINT_URL=https://mi-modelo.hf.space
   ```

2. Usar en config.yaml:
   ```yaml
   model: "mimodelo"
   ```

## ğŸ“ˆ VisualizaciÃ³n de Resultados

Los resultados se pueden visualizar en el dashboard interactivo:

```bash
cd bench/pipeline/results/dashboard
python serve_dashboard.py
# Abrir http://localhost:8000
```

El dashboard permite:
- Comparar mÃºltiples modelos
- Ver evoluciÃ³n temporal
- Analizar casos especÃ­ficos
- Exportar grÃ¡ficos

## ğŸ”§ Desarrollo y Testing

### Dataset de desarrollo
Para desarrollo rÃ¡pido, usa `ramedis-5.json`:
```yaml
dataset_path: "bench/datasets/ramedis-5.json"
```

### Modo debug
En `run.py`, activar logs detallados:
```python
logging.basicConfig(level=logging.DEBUG)
```

### Tests unitarios
```bash
pytest tests/test_bench/
```

## ğŸ“ Mejores PrÃ¡cticas

1. **Empezar pequeÃ±o**: Usar dataset de 5 casos para validar configuraciÃ³n
2. **Versionar prompts**: Guardar versiones de prompts que funcionan bien
3. **Documentar experimentos**: AÃ±adir notas en config.yaml
4. **Comparar fair**: Usar mismo dataset y prompts para todos los modelos
5. **MÃºltiples runs**: Ejecutar 3+ veces para manejar variabilidad

## ğŸ¯ Casos de Uso

- **InvestigaciÃ³n**: Comparar capacidades diagnÃ³sticas de diferentes LLMs
- **Desarrollo**: Mejorar prompts para mejor rendimiento
- **ValidaciÃ³n**: Verificar que actualizaciones no degraden performance
- **SelecciÃ³n**: Elegir el mejor modelo para producciÃ³n

## ğŸš¨ Consideraciones Importantes

- Los datasets son sintÃ©ticos/anonimizados, NO contienen datos reales de pacientes
- Los resultados son para investigaciÃ³n, NO para diagnÃ³stico clÃ­nico real
- La evaluaciÃ³n es automÃ¡tica, puede tener sesgos o limitaciones
- Siempre validar con expertos mÃ©dicos antes de conclusiones

## ğŸ”— Referencias

- [Pipeline - DocumentaciÃ³n detallada](pipeline/README.md)
- [Dashboard - GuÃ­a de visualizaciÃ³n](pipeline/results/dashboard/README.md)
- [Datasets - Origen y estructura](../data29/README.md)