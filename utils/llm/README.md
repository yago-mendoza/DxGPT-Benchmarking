# AzureLLM v4 ðŸš€

**Wrapper elegante y poderoso para Azure OpenAI API**

Una librerÃ­a diseÃ±ada para desarrolladores que quieren mÃ¡xima productividad con mÃ­nima complejidad. ConfiguraciÃ³n automÃ¡tica, esquemas JSON, plantillas reutilizables y cÃ³digo limpio.

## ðŸŽ¯ CaracterÃ­sticas Principales

- âœ… **ConfiguraciÃ³n automÃ¡tica** desde variables de entorno
- âœ… **Salida estructurada** con esquemas JSON
- âœ… **Sistema de plantillas** reutilizables
- âœ… **Procesamiento por lotes (batch)** eficiente
- âœ… **API limpia** con alias intuitivos
- âœ… **Manejo de errores** elegante
- âœ… **Compatibilidad hacia atrÃ¡s** total
- âœ… **Zero-config** para casos simples

## ðŸ“¦ InstalaciÃ³n

```bash
pip install openai python-dotenv pyyaml
```

> **Note:** La librerÃ­a requiere `openai` para Azure OpenAI y opcionalmente `python-dotenv` para variables de entorno.

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

Crea un archivo `.env` en tu proyecto:

```bash
AZURE_OPENAI_ENDPOINT=https://tu-recurso.openai.azure.com/
AZURE_OPENAI_API_KEY=tu-api-key-aqui
AZURE_OPENAI_API_VERSION=2024-02-15-preview
```

> **Tip:** La librerÃ­a carga automÃ¡ticamente las variables de entorno. No necesitas configuraciÃ³n adicional.

## ðŸš€ Inicio RÃ¡pido

### Uso BÃ¡sico

```python
from utils.llm import Azure

# Â¡ConfiguraciÃ³n automÃ¡tica desde .env!
llm = Azure("gpt-4o")
response = llm.generate("Explica quÃ© es la inteligencia artificial")
print(response)
```

### Una LÃ­nea de CÃ³digo

```python
from utils.llm import quick_generate

# Para casos sÃºper simples
response = quick_generate("Traduce 'Hello' al espaÃ±ol")
```

## ðŸ“ Ejemplos de Uso

### 1. GeneraciÃ³n BÃ¡sica con ParÃ¡metros

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

# Con temperatura baja para respuestas consistentes
response = llm.generate(
    "Resume este texto en 3 puntos clave",
    temperature=0.2,
    max_tokens=150
)
```

### 2. Salida Estructurada con JSON

```python
# Definir esquema de salida
schema = {
    "type": "object",
    "properties": {
        "resumen": {"type": "string"},
        "puntos_clave": {
            "type": "array",
            "items": {"type": "string"}
        },
        "categoria": {"type": "string"}
    },
    "required": ["resumen", "puntos_clave", "categoria"]
}

# Generar respuesta estructurada
response = llm.generate(
    "Analiza este artÃ­culo sobre IA...",
    schema=schema
)

# response es un dict con la estructura definida
print(response["resumen"])
print(response["puntos_clave"])
```

### 3. Plantillas Reutilizables

```python
# Crear plantilla con variables
traductor = llm.template(
    "Traduce '{texto}' del {idioma_origen} al {idioma_destino}",
    temperature=0.3
)

# Usar la plantilla mÃºltiples veces
resultado1 = traductor(
    texto="Hello world", 
    idioma_origen="inglÃ©s", 
    idioma_destino="espaÃ±ol"
)

resultado2 = traductor(
    texto="Bonjour", 
    idioma_origen="francÃ©s", 
    idioma_destino="espaÃ±ol"
)
```

### 4. Plantillas con Esquemas

```python
# Plantilla que siempre devuelve JSON estructurado
analizador = llm.template(
    "Analiza el sentimiento de: '{texto}'",
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {"type": "string"},
            "confianza": {"type": "number"},
            "palabras_clave": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["sentimiento", "confianza", "palabras_clave"]
    }
)

resultado = analizador(texto="Â¡Me encanta este producto!")
print(f"Sentimiento: {resultado['sentimiento']}")
print(f"Confianza: {resultado['confianza']}")
```

### 5. Procesamiento por Lotes (Batch)

```python
# Procesar mÃºltiples items en una sola llamada
items = [
    {"id": 1, "text": "Hello world"},
    {"id": 2, "text": "Good morning"},
    {"id": 3, "text": "How are you?"}
]

# Sin schema - devuelve lista de strings
responses = llm.generate(
    "Traduce cada texto al espaÃ±ol",
    batch_items=items
)
# responses = ["Hola mundo", "Buenos dÃ­as", "Â¿CÃ³mo estÃ¡s?"]

# Con schema - devuelve lista estructurada
schema = {
    "type": "object",
    "properties": {
        "original": {"type": "string"},
        "traduccion": {"type": "string"},
        "idioma": {"type": "string"}
    }
}

responses = llm.generate(
    "Traduce cada texto al espaÃ±ol y detecta el idioma original",
    batch_items=items,
    schema=schema
)
# responses = [
#     {"original": "Hello world", "traduccion": "Hola mundo", "idioma": "inglÃ©s"},
#     {"original": "Good morning", "traduccion": "Buenos dÃ­as", "idioma": "inglÃ©s"},
#     {"original": "How are you?", "traduccion": "Â¿CÃ³mo estÃ¡s?", "idioma": "inglÃ©s"}
# ]
```

## ðŸ”§ ConfiguraciÃ³n Avanzada

### ConfiguraciÃ³n Personalizada

```python
from utils.llm import LLMConfig, AzureLLM

# ConfiguraciÃ³n explÃ­cita
config = LLMConfig(
    endpoint="https://mi-recurso.openai.azure.com/",
    api_key="mi-api-key",
    deployment_name="gpt-4o",
    temperature=0.7,
    validate_schema=True  # ValidaciÃ³n strict de esquemas
)

llm = AzureLLM(config=config)
```

### ConfiguraciÃ³n desde Entorno con Overrides

```python
from utils.llm import Azure

# Usar variables de entorno pero override algunos valores
llm = Azure("gpt-4o", temperature=0.1, validate_schema=True)
```

## ðŸ“š API Reference

### Azure / AzureLLM

La clase principal para interactuar con Azure OpenAI.

```python
llm = Azure(deployment_name, *, config=None, **config_overrides)
```

**ParÃ¡metros:**
- `deployment_name`: Nombre del deployment (ej: "gpt-4o")
- `config`: Objeto LLMConfig personalizado (opcional)
- `**config_overrides`: Overrides de configuraciÃ³n

#### MÃ©todo `generate()`

```python
response = llm.generate(
    prompt,
    *,
    variables=None,
    schema=None,
    batch_items=None,
    max_tokens=None,
    temperature=None
)
```

**ParÃ¡metros:**
- `prompt`: Texto del prompt (puede contener {variables})
- `variables`: Dict con valores para sustituir en el prompt
- `schema`: Esquema JSON para salida estructurada
- `batch_items`: Lista de diccionarios para procesamiento por lotes
- `max_tokens`: LÃ­mite de tokens
- `temperature`: Nivel de creatividad (0.0-1.0)

**Retorna:**
- String para salida de texto simple
- Dict para salida estructurada con schema
- List para procesamiento por lotes

#### MÃ©todo `template()`

```python
template = llm.template(template_string, *, schema=None, **fixed_params)
```

Crea una plantilla reutilizable con parÃ¡metros fijos.

### Funciones de Conveniencia

#### `quick_generate()`

```python
from utils.llm import quick_generate

response = quick_generate(prompt, deployment_name="gpt-4o", **kwargs)
```

GeneraciÃ³n rÃ¡pida en una lÃ­nea para casos simples.

#### `create_llm()`

```python
from utils.llm import create_llm

llm = create_llm(deployment_name="gpt-4o", **config_overrides)
```

Factory function para crear instancias de LLM.

### Schema

Manejo de esquemas JSON optimizados para Azure OpenAI.

```python
from utils.llm import Schema

# Desde dict
schema = Schema.load({
    "type": "object",
    "properties": {"name": {"type": "string"}},
    "required": ["name"]
})

# Desde archivo YAML
schema = Schema.load("mi_schema.yaml")
```

## ðŸ’¡ Tips y Mejores PrÃ¡cticas

### ðŸŽ¯ OptimizaciÃ³n de Costos

```python
# Usa max_tokens para controlar costos
response = llm.generate(
    "Resume en 50 palabras...", 
    max_tokens=100  # Limita la respuesta
)

# Temperatura baja para respuestas mÃ¡s predecibles
llm = Azure("gpt-4o", temperature=0.1)
```

### ðŸ”’ Esquemas Robustos

```python
# Los esquemas se optimizan automÃ¡ticamente para Azure
schema = {
    "type": "object",
    "properties": {
        "campo": {"type": "string"}
    }
    # Azure aÃ±ade automÃ¡ticamente:
    # "additionalProperties": false
    # "required": ["campo"]
}
```

> **Note:** AzureLLM optimiza automÃ¡ticamente los esquemas para el modo strict de Azure OpenAI.

### ðŸ”„ ReutilizaciÃ³n de Instancias

```python
# âœ… Buena prÃ¡ctica: reutilizar instancia
llm = Azure("gpt-4o")
for texto in textos:
    response = llm.generate(f"Analiza: {texto}")

# âŒ Evitar: crear nueva instancia cada vez
for texto in textos:
    llm = Azure("gpt-4o")  # Ineficiente
    response = llm.generate(f"Analiza: {texto}")
```

### ðŸ§© Variables en Plantillas

```python
# âœ… Usa variables para prompts dinÃ¡micos
template = llm.template("Traduce '{texto}' al {idioma}")
result = template(texto="Hello", idioma="espaÃ±ol")

# âŒ Evitar concatenaciÃ³n manual
prompt = f"Traduce '{texto}' al {idioma}"  # Menos flexible
```

### ðŸš€ Procesamiento Eficiente por Lotes

```python
# âœ… Procesar mÃºltiples items en una sola llamada
items = [{"name": "Alice"}, {"name": "Bob"}, {"name": "Charlie"}]
greetings = llm.generate(
    "Genera un saludo personalizado para cada persona",
    batch_items=items
)

# âŒ Evitar: mÃºltiples llamadas individuales
for item in items:
    greeting = llm.generate(f"Saluda a {item['name']}")  # Ineficiente
```

> **Tip:** El procesamiento por lotes es ideal para transformaciones cortas y consistentes. Para outputs muy largos, usa lotes mÃ¡s pequeÃ±os para mantener la calidad.

### Procesador de Datos por Lotes

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

# Datos a procesar
productos = [
    {"nombre": "Laptop", "precio": 999},
    {"nombre": "Mouse", "precio": 29},
    {"nombre": "Teclado", "precio": 79}
]

# Schema para la respuesta
schema = {
    "type": "object",
    "properties": {
        "nombre": {"type": "string"},
        "categoria": {"type": "string"},
        "rango_precio": {"type": "string"},
        "descripcion_marketing": {"type": "string"}
    }
}

# Procesar todos los productos en una sola llamada
resultados = llm.generate(
    """Para cada producto, determina su categorÃ­a, rango de precio 
    (econÃ³mico/medio/premium) y genera una descripciÃ³n de marketing atractiva""",
    batch_items=productos,
    schema=schema,
    temperature=0.7
)

# Mostrar resultados
for i, resultado in enumerate(resultados):
    print(f"\nProducto {i+1}:")
    print(f"  Nombre: {resultado['nombre']}")
    print(f"  CategorÃ­a: {resultado['categoria']}")
    print(f"  Rango: {resultado['rango_precio']}")
    print(f"  Marketing: {resultado['descripcion_marketing']}")
```

## ðŸ”§ Troubleshooting

### Error: Missing AZURE_OPENAI_ENDPOINT

```bash
ValueError: Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY
```

**SoluciÃ³n:** Verifica que tienes un archivo `.env` con las variables correctas o pasa la configuraciÃ³n explÃ­citamente.

### Error: KeyError en Template

```python
KeyError: Missing template variable: 'nombre'
```

**SoluciÃ³n:** AsegÃºrate de pasar todas las variables definidas en el template:

```python
template = llm.template("Hola {nombre}")
response = template(nombre="Juan")  # âœ… Variable proporcionada
```

### Respuesta No-JSON con Schema

```
UserWarning: Model returned non-JSON despite schema constraint
```

**SoluciÃ³n:** Esto es normal. AzureLLM maneja automÃ¡ticamente casos donde el modelo no respeta el esquema y devuelve el texto raw.

### ValidaciÃ³n de Schema

```python
# Habilitar validaciÃ³n strict (requiere jsonschema)
pip install jsonschema

llm = Azure("gpt-4o", validate_schema=True)
```

## ðŸ“„ Compatibilidad hacia AtrÃ¡s

AzureLLM v4 mantiene compatibilidad total con versiones anteriores:

```python
# ParÃ¡metros antiguos aÃºn funcionan
response = llm.generate(
    "Hola {nombre}",
    prompt_vars={"nombre": "Juan"},    # Ahora se llama 'variables'
    output_schema=schema               # Ahora se llama 'schema'
)
```

> **Note:** Se recomienda usar los nuevos nombres de parÃ¡metros para cÃ³digo nuevo.

## ðŸŽ‰ Ejemplos Completos

### Analizador de Sentimientos

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

analizador = llm.template(
    """Analiza el sentimiento del siguiente texto: "{texto}"
    
    Responde con el sentimiento (positivo/negativo/neutro) y una puntuaciÃ³n de confianza del 0 al 1.""",
    
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {
                "type": "string",
                "enum": ["positivo", "negativo", "neutro"]
            },
            "confianza": {
                "type": "number",
                "minimum": 0,
                "maximum": 1
            },
            "razon": {"type": "string"}
        },
        "required": ["sentimiento", "confianza", "razon"]
    },
    temperature=0.2
)

# Usar el analizador
textos = [
    "Â¡Me encanta este producto!",
    "El servicio fue terrible",
    "El clima estÃ¡ nublado hoy"
]

for texto in textos:
    resultado = analizador(texto=texto)
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['sentimiento']} ({resultado['confianza']:.2f})")
    print(f"RazÃ³n: {resultado['razon']}\n")
```

---

**AzureLLM v4** - Desarrollado con ðŸ’› para mÃ¡xima productividad y mÃ­nima fricciÃ³n.
