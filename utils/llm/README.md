LLM Utils v4 üöÄ
Wrapper elegante y poderoso para m√∫ltiples proveedores de LLM
Una librer√≠a dise√±ada para desarrolladores que quieren m√°xima productividad con m√≠nima complejidad. Soporta Azure OpenAI y Hugging Face con una interfaz unificada. Configuraci√≥n autom√°tica, esquemas JSON, plantillas reutilizables y c√≥digo limpio.
üéØ Caracter√≠sticas Principales

‚úÖ Multi-proveedor - Azure OpenAI y Hugging Face (extensible)
‚úÖ Configuraci√≥n autom√°tica desde variables de entorno
‚úÖ Salida estructurada con esquemas JSON
‚úÖ Sistema de plantillas reutilizables
‚úÖ Procesamiento por lotes (batch) eficiente
‚úÖ API unificada con alias intuitivos
‚úÖ Manejo de errores elegante
‚úÖ Compatibilidad hacia atr√°s total
‚úÖ Zero-config para casos simples

üì¶ Instalaci√≥n
bash# Para Azure OpenAI
pip install openai python-dotenv pyyaml

# Para Hugging Face
pip install requests python-dotenv pyyaml

Note: python-dotenv es opcional pero recomendado para cargar variables de entorno autom√°ticamente.

‚öôÔ∏è Configuraci√≥n
Variables de Entorno
Crea un archivo .env en tu proyecto:
Para Azure OpenAI
bashAZURE_OPENAI_ENDPOINT=https://tu-recurso.openai.azure.com/
AZURE_OPENAI_API_KEY=tu-api-key-aqui
AZURE_OPENAI_API_VERSION=2024-02-15-preview
Para Hugging Face
bashHF_TOKEN=tu-token-de-hugging-face

# Endpoints por modelo - el nombre en may√∫sculas ser√° el identificador del modelo
JONSNOW_ENDPOINT_URL=https://tu-deployment-jonsnow.hf.space
MISTRAL_ENDPOINT_URL=https://tu-deployment-mistral.hf.space
LLAMA_ENDPOINT_URL=https://tu-deployment-llama.hf.space

Important: Para Hugging Face, el patr√≥n es {MODELO}_ENDPOINT_URL donde {MODELO} es el nombre que usar√°s al crear la instancia (en min√∫sculas). Por ejemplo, JONSNOW_ENDPOINT_URL se usa con HuggingLLM("jonsnow").

üöÄ Inicio R√°pido
Azure OpenAI
pythonfrom utils.llm import Azure

# ¬°Configuraci√≥n autom√°tica desde .env!
llm = Azure("gpt-4o")
response = llm.generate("Explica qu√© es la inteligencia artificial")
print(response)
Hugging Face
pythonfrom utils.llm import HuggingLLM

# Usa JONSNOW_ENDPOINT_URL desde .env
llm = HuggingLLM("jonsnow")
response = llm.generate("Diagn√≥stico para paciente con dolor de pecho")
print(response)
Una L√≠nea de C√≥digo
pythonfrom utils.llm import quick_generate

# Para Azure (por defecto)
response = quick_generate("Traduce 'Hello' al espa√±ol")

# Para Hugging Face
from utils.llm.hugging import quick_generate as hf_generate
response = hf_generate("Analiza estos s√≠ntomas", deployment_name="jonsnow")
üìù Ejemplos de Uso
Los ejemplos funcionan id√©nticamente para ambos proveedores. Solo cambia la importaci√≥n:
python# Para Azure
from utils.llm import Azure as LLM

# Para Hugging Face
from utils.llm import HuggingLLM as LLM

# El resto del c√≥digo es id√©ntico
llm = LLM("tu-modelo")
1. Generaci√≥n B√°sica con Par√°metros
pythonllm = LLM("gpt-4o")  # o "jonsnow" para HF

# Con temperatura baja para respuestas consistentes
response = llm.generate(
    "Resume este texto en 3 puntos clave",
    temperature=0.2,
    max_tokens=150
)
2. Salida Estructurada con JSON
python# Definir esquema de salida
schema = {
    "type": "object",
    "properties": {
        "resumen": {"type": "string"},
        "puntos_clave": {
            "type": "array",
            "items": {"type": "string"}
        },
        "categoria": {"type": "string"}
    },
    "required": ["resumen", "puntos_clave", "categoria"]
}

# Generar respuesta estructurada
response = llm.generate(
    "Analiza este art√≠culo sobre IA...",
    schema=schema
)

# response es un dict con la estructura definida
print(response["resumen"])
print(response["puntos_clave"])
3. Plantillas Reutilizables
python# Crear plantilla con variables
traductor = llm.template(
    "Traduce '{texto}' del {idioma_origen} al {idioma_destino}",
    temperature=0.3
)

# Usar la plantilla m√∫ltiples veces
resultado1 = traductor(
    texto="Hello world", 
    idioma_origen="ingl√©s", 
    idioma_destino="espa√±ol"
)

resultado2 = traductor(
    texto="Bonjour", 
    idioma_origen="franc√©s", 
    idioma_destino="espa√±ol"
)
4. Plantillas con Esquemas
python# Plantilla que siempre devuelve JSON estructurado
analizador = llm.template(
    "Analiza el sentimiento de: '{texto}'",
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {"type": "string"},
            "confianza": {"type": "number"},
            "palabras_clave": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["sentimiento", "confianza", "palabras_clave"]
    }
)

resultado = analizador(texto="¬°Me encanta este producto!")
print(f"Sentimiento: {resultado['sentimiento']}")
print(f"Confianza: {resultado['confianza']}")
5. Procesamiento por Lotes (Batch)
python# Procesar m√∫ltiples items en una sola llamada
items = [
    {"id": 1, "text": "Hello world"},
    {"id": 2, "text": "Good morning"},
    {"id": 3, "text": "How are you?"}
]

# Sin schema - devuelve lista de strings
responses = llm.generate(
    "Traduce cada texto al espa√±ol",
    batch_items=items
)
# responses = ["Hola mundo", "Buenos d√≠as", "¬øC√≥mo est√°s?"]

# Con schema - devuelve lista estructurada
schema = {
    "type": "object",
    "properties": {
        "original": {"type": "string"},
        "traduccion": {"type": "string"},
        "idioma": {"type": "string"}
    }
}

responses = llm.generate(
    "Traduce cada texto al espa√±ol y detecta el idioma original",
    batch_items=items,
    schema=schema
)
# responses = [
#     {"original": "Hello world", "traduccion": "Hola mundo", "idioma": "ingl√©s"},
#     {"original": "Good morning", "traduccion": "Buenos d√≠as", "idioma": "ingl√©s"},
#     {"original": "How are you?", "traduccion": "¬øC√≥mo est√°s?", "idioma": "ingl√©s"}
# ]
üîß Configuraci√≥n Avanzada
Configuraci√≥n Personalizada
pythonfrom utils.llm import LLMConfig, AzureLLM, HuggingLLM

# Configuraci√≥n expl√≠cita para Azure
config = LLMConfig(
    endpoint="https://mi-recurso.openai.azure.com/",
    api_key="mi-api-key",
    deployment_name="gpt-4o",
    temperature=0.7,
    validate_schema=True
)
llm = AzureLLM(config=config)

# Configuraci√≥n expl√≠cita para Hugging Face
config = LLMConfig(
    endpoint="https://mi-modelo.hf.space",
    api_key="mi-hf-token",
    deployment_name="mi-modelo",
    temperature=0.7
)
llm = HuggingLLM(config=config)
Configuraci√≥n desde Entorno con Overrides
python# Azure
from utils.llm import Azure
llm = Azure("gpt-4o", temperature=0.1, validate_schema=True)

# Hugging Face
from utils.llm import HuggingLLM
llm = HuggingLLM("jonsnow", temperature=0.1)
üîå A√±adir Nuevos Proveedores
Para a√±adir soporte para un nuevo proveedor de LLM:

Crea un nuevo archivo (ej: anthropic.py) en utils/llm/
Implementa la misma interfaz que AzureLLM:

Clase principal con m√©todos generate() y template()
Mismos par√°metros y comportamiento
Funciones de conveniencia (create_llm, quick_generate)


Exp√≥n aliases compatibles al final del archivo:
python# Tu implementaci√≥n
class AnthropicLLM:
    # ... implementaci√≥n ...

# Aliases para compatibilidad
AzureLLM = AnthropicLLM
Azure = AnthropicLLM
create_azure_llm = create_llm
quick_generate_azure = quick_generate

Actualiza __init__.py para incluir tu proveedor:
pythonfrom .anthropic import (
    Anthropic,
    AnthropicLLM,
    # ... otros exports
)


Esto garantiza que tu proveedor sea un drop-in replacement completo.
üìö API Reference
Clase Principal (Azure / HuggingLLM)
pythonllm = ProviderLLM(deployment_name, *, config=None, **config_overrides)
Par√°metros:

deployment_name:

Azure: Nombre del deployment (ej: "gpt-4o")
Hugging Face: Nombre del modelo en min√∫sculas (ej: "jonsnow" para JONSNOW_ENDPOINT_URL)


config: Objeto LLMConfig personalizado (opcional)
**config_overrides: Overrides de configuraci√≥n

M√©todo generate()
pythonresponse = llm.generate(
    prompt,
    *,
    variables=None,
    schema=None,
    batch_items=None,
    max_tokens=None,
    temperature=None
)
Par√°metros:

prompt: Texto del prompt (puede contener {variables})
variables: Dict con valores para sustituir en el prompt
schema: Esquema JSON para salida estructurada
batch_items: Lista de diccionarios para procesamiento por lotes
max_tokens: L√≠mite de tokens
temperature: Nivel de creatividad (0.0-1.0)

Retorna:

String para salida de texto simple
Dict para salida estructurada con schema
List para procesamiento por lotes

M√©todo template()
pythontemplate = llm.template(template_string, *, schema=None, **fixed_params)
Crea una plantilla reutilizable con par√°metros fijos.
Funciones de Conveniencia
quick_generate()
python# Azure
from utils.llm import quick_generate
response = quick_generate(prompt, deployment_name="gpt-4o", **kwargs)

# Hugging Face
from utils.llm.hugging import quick_generate
response = quick_generate(prompt, deployment_name="jonsnow", **kwargs)
create_llm()
python# Azure
from utils.llm import create_llm
llm = create_llm(deployment_name="gpt-4o", **config_overrides)

# Hugging Face
from utils.llm.hugging import create_llm
llm = create_llm(deployment_name="jonsnow", **config_overrides)
Schema
Manejo de esquemas JSON optimizados para cada proveedor.
pythonfrom utils.llm import Schema

# Desde dict
schema = Schema.load({
    "type": "object",
    "properties": {"name": {"type": "string"}},
    "required": ["name"]
})

# Desde archivo YAML
schema = Schema.load("mi_schema.yaml")
üí° Tips y Mejores Pr√°cticas
üéØ Configuraci√≥n de Modelos Hugging Face
python# Nombre del modelo = clave del endpoint en min√∫sculas
# JONSNOW_ENDPOINT_URL ‚Üí HuggingLLM("jonsnow")
# MISTRAL_ENDPOINT_URL ‚Üí HuggingLLM("mistral")
# MI_MODELO_ENDPOINT_URL ‚Üí HuggingLLM("mi_modelo")

# El token HF_TOKEN es compartido por todos los modelos
üîí Esquemas seg√∫n Proveedor
python# Azure optimiza autom√°ticamente para strict mode
# Hugging Face usa TGI grammar constraints

# El mismo esquema funciona en ambos
schema = {
    "type": "object",
    "properties": {
        "campo": {"type": "string"}
    }
}
üîÑ Migraci√≥n entre Proveedores
python# Cambiar de proveedor es tan simple como cambiar el import
# De Azure a Hugging Face:
# from utils.llm import Azure as LLM
from utils.llm import HuggingLLM as LLM

# El resto del c√≥digo permanece id√©ntico
llm = LLM("modelo")
response = llm.generate("prompt", schema=schema)
üß© Variables de Entorno Organizadas
bash# .env organizado por proveedor

# === Azure OpenAI ===
AZURE_OPENAI_ENDPOINT=https://...
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# === Hugging Face ===
HF_TOKEN=hf_...

# Modelos m√©dicos
JONSNOW_ENDPOINT_URL=https://...
MEDLLAMA_ENDPOINT_URL=https://...

# Modelos generales
MISTRAL_ENDPOINT_URL=https://...
LLAMA_ENDPOINT_URL=https://...
üöÄ Rendimiento por Proveedor
python# Azure: Mejor para aplicaciones en producci√≥n
# - L√≠mites de rate m√°s altos
# - SLAs empresariales
# - Menor latencia

# Hugging Face: Ideal para prototipos y modelos especializados
# - Modelos open source
# - Personalizaci√≥n completa
# - Endpoints privados
üîß Troubleshooting
Error: Missing Endpoint Variables
bash# Azure
ValueError: Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY

# Hugging Face
ValueError: Missing JONSNOW_ENDPOINT_URL or HF_TOKEN
Soluci√≥n: Verifica tu archivo .env y que los nombres de las variables coincidan con el patr√≥n esperado.
Error: Modelo No Encontrado (Hugging Face)
python# Si usas HuggingLLM("mimodelo")
# Debe existir MIMODELO_ENDPOINT_URL en .env
Soluci√≥n: El nombre del modelo en el c√≥digo debe coincidir (en may√∫sculas) con el prefijo de _ENDPOINT_URL.
Respuesta No-JSON con Schema
UserWarning: Model returned non-JSON despite schema constraint
Soluci√≥n: Normal en ambos proveedores. La librer√≠a maneja autom√°ticamente estos casos y devuelve el texto raw.
üìÑ Compatibilidad
Ambos proveedores mantienen compatibilidad total:
python# Estos par√°metros legacy funcionan en ambos
response = llm.generate(
    "Hola {nombre}",
    prompt_vars={"nombre": "Juan"},    # Usar 'variables' en c√≥digo nuevo
    output_schema=schema               # Usar 'schema' en c√≥digo nuevo
)
üéâ Ejemplo Completo Multi-Proveedor
pythonfrom utils.llm import Azure, HuggingLLM

# Configurar ambos proveedores
azure_llm = Azure("gpt-4o")
hf_llm = HuggingLLM("jonsnow")

# Mismo esquema para ambos
schema = {
    "type": "object",
    "properties": {
        "diagnostico": {"type": "string"},
        "confianza": {"type": "number"},
        "severidad": {"type": "string"}
    },
    "required": ["diagnostico", "confianza", "severidad"]
}

# Mismo prompt
sintomas = "Paciente con dolor tor√°cico, disnea y sudoraci√≥n"

# Comparar resultados
resultado_azure = azure_llm.generate(
    f"Analiza estos s√≠ntomas: {sintomas}",
    schema=schema
)

resultado_hf = hf_llm.generate(
    f"Analiza estos s√≠ntomas: {sintomas}",
    schema=schema
)

print("Azure GPT-4:", resultado_azure)
print("JohnSnow HF:", resultado_hf)


# AzureLLM v4 üöÄ

**Wrapper elegante y poderoso para Azure OpenAI API**

Una librer√≠a dise√±ada para desarrolladores que quieren m√°xima productividad con m√≠nima complejidad. Configuraci√≥n autom√°tica, esquemas JSON, plantillas reutilizables y c√≥digo limpio.

## üéØ Caracter√≠sticas Principales

- ‚úÖ **Configuraci√≥n autom√°tica** desde variables de entorno
- ‚úÖ **Salida estructurada** con esquemas JSON
- ‚úÖ **Sistema de plantillas** reutilizables
- ‚úÖ **Procesamiento por lotes (batch)** eficiente
- ‚úÖ **API limpia** con alias intuitivos
- ‚úÖ **Manejo de errores** elegante
- ‚úÖ **Compatibilidad hacia atr√°s** total
- ‚úÖ **Zero-config** para casos simples

## üì¶ Instalaci√≥n

```bash
pip install openai python-dotenv pyyaml
```

> **Note:** La librer√≠a requiere `openai` para Azure OpenAI y opcionalmente `python-dotenv` para variables de entorno.

## ‚öôÔ∏è Configuraci√≥n

### Variables de Entorno

Crea un archivo `.env` en tu proyecto:

```bash
AZURE_OPENAI_ENDPOINT=https://tu-recurso.openai.azure.com/
AZURE_OPENAI_API_KEY=tu-api-key-aqui
AZURE_OPENAI_API_VERSION=2024-02-15-preview
```

> **Tip:** La librer√≠a carga autom√°ticamente las variables de entorno. No necesitas configuraci√≥n adicional.

## üöÄ Inicio R√°pido

### Uso B√°sico

```python
from utils.llm import Azure

# ¬°Configuraci√≥n autom√°tica desde .env!
llm = Azure("gpt-4o")
response = llm.generate("Explica qu√© es la inteligencia artificial")
print(response)
```

### Una L√≠nea de C√≥digo

```python
from utils.llm import quick_generate

# Para casos s√∫per simples
response = quick_generate("Traduce 'Hello' al espa√±ol")
```

## üìù Ejemplos de Uso

### 1. Generaci√≥n B√°sica con Par√°metros

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

# Con temperatura baja para respuestas consistentes
response = llm.generate(
    "Resume este texto en 3 puntos clave",
    temperature=0.2,
    max_tokens=150
)
```

### 2. Salida Estructurada con JSON

```python
# Definir esquema de salida
schema = {
    "type": "object",
    "properties": {
        "resumen": {"type": "string"},
        "puntos_clave": {
            "type": "array",
            "items": {"type": "string"}
        },
        "categoria": {"type": "string"}
    },
    "required": ["resumen", "puntos_clave", "categoria"]
}

# Generar respuesta estructurada
response = llm.generate(
    "Analiza este art√≠culo sobre IA...",
    schema=schema
)

# response es un dict con la estructura definida
print(response["resumen"])
print(response["puntos_clave"])
```

### 3. Plantillas Reutilizables

```python
# Crear plantilla con variables
traductor = llm.template(
    "Traduce '{texto}' del {idioma_origen} al {idioma_destino}",
    temperature=0.3
)

# Usar la plantilla m√∫ltiples veces
resultado1 = traductor(
    texto="Hello world", 
    idioma_origen="ingl√©s", 
    idioma_destino="espa√±ol"
)

resultado2 = traductor(
    texto="Bonjour", 
    idioma_origen="franc√©s", 
    idioma_destino="espa√±ol"
)
```

### 4. Plantillas con Esquemas

```python
# Plantilla que siempre devuelve JSON estructurado
analizador = llm.template(
    "Analiza el sentimiento de: '{texto}'",
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {"type": "string"},
            "confianza": {"type": "number"},
            "palabras_clave": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["sentimiento", "confianza", "palabras_clave"]
    }
)

resultado = analizador(texto="¬°Me encanta este producto!")
print(f"Sentimiento: {resultado['sentimiento']}")
print(f"Confianza: {resultado['confianza']}")
```

### 5. Procesamiento por Lotes (Batch)

```python
# Procesar m√∫ltiples items en una sola llamada
items = [
    {"id": 1, "text": "Hello world"},
    {"id": 2, "text": "Good morning"},
    {"id": 3, "text": "How are you?"}
]

# Sin schema - devuelve lista de strings
responses = llm.generate(
    "Traduce cada texto al espa√±ol",
    batch_items=items
)
# responses = ["Hola mundo", "Buenos d√≠as", "¬øC√≥mo est√°s?"]

# Con schema - devuelve lista estructurada
schema = {
    "type": "object",
    "properties": {
        "original": {"type": "string"},
        "traduccion": {"type": "string"},
        "idioma": {"type": "string"}
    }
}

responses = llm.generate(
    "Traduce cada texto al espa√±ol y detecta el idioma original",
    batch_items=items,
    schema=schema
)
# responses = [
#     {"original": "Hello world", "traduccion": "Hola mundo", "idioma": "ingl√©s"},
#     {"original": "Good morning", "traduccion": "Buenos d√≠as", "idioma": "ingl√©s"},
#     {"original": "How are you?", "traduccion": "¬øC√≥mo est√°s?", "idioma": "ingl√©s"}
# ]
```

## üîß Configuraci√≥n Avanzada

### Configuraci√≥n Personalizada

```python
from utils.llm import LLMConfig, AzureLLM

# Configuraci√≥n expl√≠cita
config = LLMConfig(
    endpoint="https://mi-recurso.openai.azure.com/",
    api_key="mi-api-key",
    deployment_name="gpt-4o",
    temperature=0.7,
    validate_schema=True  # Validaci√≥n strict de esquemas
)

llm = AzureLLM(config=config)
```

### Configuraci√≥n desde Entorno con Overrides

```python
from utils.llm import Azure

# Usar variables de entorno pero override algunos valores
llm = Azure("gpt-4o", temperature=0.1, validate_schema=True)
```

## üìö API Reference

### Azure / AzureLLM

La clase principal para interactuar con Azure OpenAI.

```python
llm = Azure(deployment_name, *, config=None, **config_overrides)
```

**Par√°metros:**
- `deployment_name`: Nombre del deployment (ej: "gpt-4o")
- `config`: Objeto LLMConfig personalizado (opcional)
- `**config_overrides`: Overrides de configuraci√≥n

#### M√©todo `generate()`

```python
response = llm.generate(
    prompt,
    *,
    variables=None,
    schema=None,
    batch_items=None,
    max_tokens=None,
    temperature=None
)
```

**Par√°metros:**
- `prompt`: Texto del prompt (puede contener {variables})
- `variables`: Dict con valores para sustituir en el prompt
- `schema`: Esquema JSON para salida estructurada
- `batch_items`: Lista de diccionarios para procesamiento por lotes
- `max_tokens`: L√≠mite de tokens
- `temperature`: Nivel de creatividad (0.0-1.0)

**Retorna:**
- String para salida de texto simple
- Dict para salida estructurada con schema
- List para procesamiento por lotes

#### M√©todo `template()`

```python
template = llm.template(template_string, *, schema=None, **fixed_params)
```

Crea una plantilla reutilizable con par√°metros fijos.

### Funciones de Conveniencia

#### `quick_generate()`

```python
from utils.llm import quick_generate

response = quick_generate(prompt, deployment_name="gpt-4o", **kwargs)
```

Generaci√≥n r√°pida en una l√≠nea para casos simples.

#### `create_llm()`

```python
from utils.llm import create_llm

llm = create_llm(deployment_name="gpt-4o", **config_overrides)
```

Factory function para crear instancias de LLM.

### Schema

Manejo de esquemas JSON optimizados para Azure OpenAI.

```python
from utils.llm import Schema

# Desde dict
schema = Schema.load({
    "type": "object",
    "properties": {"name": {"type": "string"}},
    "required": ["name"]
})

# Desde archivo YAML
schema = Schema.load("mi_schema.yaml")
```

## üí° Tips y Mejores Pr√°cticas

### üéØ Optimizaci√≥n de Costos

```python
# Usa max_tokens para controlar costos
response = llm.generate(
    "Resume en 50 palabras...", 
    max_tokens=100  # Limita la respuesta
)

# Temperatura baja para respuestas m√°s predecibles
llm = Azure("gpt-4o", temperature=0.1)
```

### üîí Esquemas Robustos

```python
# Los esquemas se optimizan autom√°ticamente para Azure
schema = {
    "type": "object",
    "properties": {
        "campo": {"type": "string"}
    }
    # Azure a√±ade autom√°ticamente:
    # "additionalProperties": false
    # "required": ["campo"]
}
```

> **Note:** AzureLLM optimiza autom√°ticamente los esquemas para el modo strict de Azure OpenAI.

### üîÑ Reutilizaci√≥n de Instancias

```python
# ‚úÖ Buena pr√°ctica: reutilizar instancia
llm = Azure("gpt-4o")
for texto in textos:
    response = llm.generate(f"Analiza: {texto}")

# ‚ùå Evitar: crear nueva instancia cada vez
for texto in textos:
    llm = Azure("gpt-4o")  # Ineficiente
    response = llm.generate(f"Analiza: {texto}")
```

### üß© Variables en Plantillas

```python
# ‚úÖ Usa variables para prompts din√°micos
template = llm.template("Traduce '{texto}' al {idioma}")
result = template(texto="Hello", idioma="espa√±ol")

# ‚ùå Evitar concatenaci√≥n manual
prompt = f"Traduce '{texto}' al {idioma}"  # Menos flexible
```

### üöÄ Procesamiento Eficiente por Lotes

```python
# ‚úÖ Procesar m√∫ltiples items en una sola llamada
items = [{"name": "Alice"}, {"name": "Bob"}, {"name": "Charlie"}]
greetings = llm.generate(
    "Genera un saludo personalizado para cada persona",
    batch_items=items
)

# ‚ùå Evitar: m√∫ltiples llamadas individuales
for item in items:
    greeting = llm.generate(f"Saluda a {item['name']}")  # Ineficiente
```

> **Tip:** El procesamiento por lotes es ideal para transformaciones cortas y consistentes. Para outputs muy largos, usa lotes m√°s peque√±os para mantener la calidad.

### Procesador de Datos por Lotes

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

# Datos a procesar
productos = [
    {"nombre": "Laptop", "precio": 999},
    {"nombre": "Mouse", "precio": 29},
    {"nombre": "Teclado", "precio": 79}
]

# Schema para la respuesta
schema = {
    "type": "object",
    "properties": {
        "nombre": {"type": "string"},
        "categoria": {"type": "string"},
        "rango_precio": {"type": "string"},
        "descripcion_marketing": {"type": "string"}
    }
}

# Procesar todos los productos en una sola llamada
resultados = llm.generate(
    """Para cada producto, determina su categor√≠a, rango de precio 
    (econ√≥mico/medio/premium) y genera una descripci√≥n de marketing atractiva""",
    batch_items=productos,
    schema=schema,
    temperature=0.7
)

# Mostrar resultados
for i, resultado in enumerate(resultados):
    print(f"\nProducto {i+1}:")
    print(f"  Nombre: {resultado['nombre']}")
    print(f"  Categor√≠a: {resultado['categoria']}")
    print(f"  Rango: {resultado['rango_precio']}")
    print(f"  Marketing: {resultado['descripcion_marketing']}")
```

## üîß Troubleshooting

### Error: Missing AZURE_OPENAI_ENDPOINT

```bash
ValueError: Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY
```

**Soluci√≥n:** Verifica que tienes un archivo `.env` con las variables correctas o pasa la configuraci√≥n expl√≠citamente.

### Error: KeyError en Template

```python
KeyError: Missing template variable: 'nombre'
```

**Soluci√≥n:** Aseg√∫rate de pasar todas las variables definidas en el template:

```python
template = llm.template("Hola {nombre}")
response = template(nombre="Juan")  # ‚úÖ Variable proporcionada
```

### Respuesta No-JSON con Schema

```
UserWarning: Model returned non-JSON despite schema constraint
```

**Soluci√≥n:** Esto es normal. AzureLLM maneja autom√°ticamente casos donde el modelo no respeta el esquema y devuelve el texto raw.

### Validaci√≥n de Schema

```python
# Habilitar validaci√≥n strict (requiere jsonschema)
pip install jsonschema

llm = Azure("gpt-4o", validate_schema=True)
```

## üìÑ Compatibilidad hacia Atr√°s

AzureLLM v4 mantiene compatibilidad total con versiones anteriores:

```python
# Par√°metros antiguos a√∫n funcionan
response = llm.generate(
    "Hola {nombre}",
    prompt_vars={"nombre": "Juan"},    # Ahora se llama 'variables'
    output_schema=schema               # Ahora se llama 'schema'
)
```

> **Note:** Se recomienda usar los nuevos nombres de par√°metros para c√≥digo nuevo.

## üéâ Ejemplos Completos

### Analizador de Sentimientos

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

analizador = llm.template(
    """Analiza el sentimiento del siguiente texto: "{texto}"
    
    Responde con el sentimiento (positivo/negativo/neutro) y una puntuaci√≥n de confianza del 0 al 1.""",
    
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {
                "type": "string",
                "enum": ["positivo", "negativo", "neutro"]
            },
            "confianza": {
                "type": "number",
                "minimum": 0,
                "maximum": 1
            },
            "razon": {"type": "string"}
        },
        "required": ["sentimiento", "confianza", "razon"]
    },
    temperature=0.2
)

# Usar el analizador
textos = [
    "¬°Me encanta este producto!",
    "El servicio fue terrible",
    "El clima est√° nublado hoy"
]

for texto in textos:
    resultado = analizador(texto=texto)
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['sentimiento']} ({resultado['confianza']:.2f})")
    print(f"Raz√≥n: {resultado['razon']}\n")
```