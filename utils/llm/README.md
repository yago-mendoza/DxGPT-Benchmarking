LLM Utils v4 ðŸš€
Wrapper elegante y poderoso para mÃºltiples proveedores de LLM
Una librerÃ­a diseÃ±ada para desarrolladores que quieren mÃ¡xima productividad con mÃ­nima complejidad. Soporta Azure OpenAI y Hugging Face con una interfaz unificada. ConfiguraciÃ³n automÃ¡tica, esquemas JSON, plantillas reutilizables y cÃ³digo limpio.
ðŸŽ¯ CaracterÃ­sticas Principales

âœ… Multi-proveedor - Azure OpenAI y Hugging Face (extensible)
âœ… ConfiguraciÃ³n automÃ¡tica desde variables de entorno
âœ… Salida estructurada con esquemas JSON
âœ… Sistema de plantillas reutilizables
âœ… Procesamiento por lotes (batch) eficiente
âœ… API unificada con alias intuitivos
âœ… Manejo de errores elegante
âœ… Compatibilidad hacia atrÃ¡s total
âœ… Zero-config para casos simples

ðŸ“¦ InstalaciÃ³n
bash# Para Azure OpenAI
pip install openai python-dotenv pyyaml

# Para Hugging Face
pip install requests python-dotenv pyyaml

Note: python-dotenv es opcional pero recomendado para cargar variables de entorno automÃ¡ticamente.

âš™ï¸ ConfiguraciÃ³n
Variables de Entorno
Crea un archivo .env en tu proyecto:
Para Azure OpenAI
bashAZURE_OPENAI_ENDPOINT=https://tu-recurso.openai.azure.com/
AZURE_OPENAI_API_KEY=tu-api-key-aqui
AZURE_OPENAI_API_VERSION=2024-02-15-preview
Para Hugging Face
bashHF_TOKEN=tu-token-de-hugging-face

# Endpoints por modelo - el nombre en mayÃºsculas serÃ¡ el identificador del modelo
JONSNOW_ENDPOINT_URL=https://tu-deployment-jonsnow.hf.space
MISTRAL_ENDPOINT_URL=https://tu-deployment-mistral.hf.space
LLAMA_ENDPOINT_URL=https://tu-deployment-llama.hf.space

Important: Para Hugging Face, el patrÃ³n es {MODELO}_ENDPOINT_URL donde {MODELO} es el nombre que usarÃ¡s al crear la instancia (en minÃºsculas). Por ejemplo, JONSNOW_ENDPOINT_URL se usa con HuggingLLM("jonsnow").

ðŸš€ Inicio RÃ¡pido
Azure OpenAI
pythonfrom utils.llm import Azure

# Â¡ConfiguraciÃ³n automÃ¡tica desde .env!
llm = Azure("gpt-4o")
response = llm.generate("Explica quÃ© es la inteligencia artificial")
print(response)
Hugging Face
pythonfrom utils.llm import HuggingLLM

# Usa JONSNOW_ENDPOINT_URL desde .env
llm = HuggingLLM("jonsnow")
response = llm.generate("DiagnÃ³stico para paciente con dolor de pecho")
print(response)
Una LÃ­nea de CÃ³digo
pythonfrom utils.llm import quick_generate

# Para Azure (por defecto)
response = quick_generate("Traduce 'Hello' al espaÃ±ol")

# Para Hugging Face
from utils.llm.hugging import quick_generate as hf_generate
response = hf_generate("Analiza estos sÃ­ntomas", deployment_name="jonsnow")
ðŸ“ Ejemplos de Uso
Los ejemplos funcionan idÃ©nticamente para ambos proveedores. Solo cambia la importaciÃ³n:
python# Para Azure
from utils.llm import Azure as LLM

# Para Hugging Face
from utils.llm import HuggingLLM as LLM

# El resto del cÃ³digo es idÃ©ntico
llm = LLM("tu-modelo")
1. GeneraciÃ³n BÃ¡sica con ParÃ¡metros
pythonllm = LLM("gpt-4o")  # o "jonsnow" para HF

# Con temperatura baja para respuestas consistentes
response = llm.generate(
    "Resume este texto en 3 puntos clave",
    temperature=0.2,
    max_tokens=150
)
2. Salida Estructurada con JSON
python# Definir esquema de salida
schema = {
    "type": "object",
    "properties": {
        "resumen": {"type": "string"},
        "puntos_clave": {
            "type": "array",
            "items": {"type": "string"}
        },
        "categoria": {"type": "string"}
    },
    "required": ["resumen", "puntos_clave", "categoria"]
}

# Generar respuesta estructurada
response = llm.generate(
    "Analiza este artÃ­culo sobre IA...",
    schema=schema
)

# response es un dict con la estructura definida
print(response["resumen"])
print(response["puntos_clave"])
3. Plantillas Reutilizables
python# Crear plantilla con variables
traductor = llm.template(
    "Traduce '{texto}' del {idioma_origen} al {idioma_destino}",
    temperature=0.3
)

# Usar la plantilla mÃºltiples veces
resultado1 = traductor(
    texto="Hello world", 
    idioma_origen="inglÃ©s", 
    idioma_destino="espaÃ±ol"
)

resultado2 = traductor(
    texto="Bonjour", 
    idioma_origen="francÃ©s", 
    idioma_destino="espaÃ±ol"
)
4. Plantillas con Esquemas
python# Plantilla que siempre devuelve JSON estructurado
analizador = llm.template(
    "Analiza el sentimiento de: '{texto}'",
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {"type": "string"},
            "confianza": {"type": "number"},
            "palabras_clave": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["sentimiento", "confianza", "palabras_clave"]
    }
)

resultado = analizador(texto="Â¡Me encanta este producto!")
print(f"Sentimiento: {resultado['sentimiento']}")
print(f"Confianza: {resultado['confianza']}")
5. Procesamiento por Lotes (Batch)
python# Procesar mÃºltiples items en una sola llamada
items = [
    {"id": 1, "text": "Hello world"},
    {"id": 2, "text": "Good morning"},
    {"id": 3, "text": "How are you?"}
]

# Sin schema - devuelve lista de strings
responses = llm.generate(
    "Traduce cada texto al espaÃ±ol",
    batch_items=items
)
# responses = ["Hola mundo", "Buenos dÃ­as", "Â¿CÃ³mo estÃ¡s?"]

# Con schema - devuelve lista estructurada
schema = {
    "type": "object",
    "properties": {
        "original": {"type": "string"},
        "traduccion": {"type": "string"},
        "idioma": {"type": "string"}
    }
}

responses = llm.generate(
    "Traduce cada texto al espaÃ±ol y detecta el idioma original",
    batch_items=items,
    schema=schema
)
# responses = [
#     {"original": "Hello world", "traduccion": "Hola mundo", "idioma": "inglÃ©s"},
#     {"original": "Good morning", "traduccion": "Buenos dÃ­as", "idioma": "inglÃ©s"},
#     {"original": "How are you?", "traduccion": "Â¿CÃ³mo estÃ¡s?", "idioma": "inglÃ©s"}
# ]
ðŸ”§ ConfiguraciÃ³n Avanzada
ConfiguraciÃ³n Personalizada
pythonfrom utils.llm import LLMConfig, AzureLLM, HuggingLLM

# ConfiguraciÃ³n explÃ­cita para Azure
config = LLMConfig(
    endpoint="https://mi-recurso.openai.azure.com/",
    api_key="mi-api-key",
    deployment_name="gpt-4o",
    temperature=0.7,
    validate_schema=True
)
llm = AzureLLM(config=config)

# ConfiguraciÃ³n explÃ­cita para Hugging Face
config = LLMConfig(
    endpoint="https://mi-modelo.hf.space",
    api_key="mi-hf-token",
    deployment_name="mi-modelo",
    temperature=0.7
)
llm = HuggingLLM(config=config)
ConfiguraciÃ³n desde Entorno con Overrides
python# Azure
from utils.llm import Azure
llm = Azure("gpt-4o", temperature=0.1, validate_schema=True)

# Hugging Face
from utils.llm import HuggingLLM
llm = HuggingLLM("jonsnow", temperature=0.1)
ðŸ”Œ AÃ±adir Nuevos Proveedores
Para aÃ±adir soporte para un nuevo proveedor de LLM:

Crea un nuevo archivo (ej: anthropic.py) en utils/llm/
Implementa la misma interfaz que AzureLLM:

Clase principal con mÃ©todos generate() y template()
Mismos parÃ¡metros y comportamiento
Funciones de conveniencia (create_llm, quick_generate)


ExpÃ³n aliases compatibles al final del archivo:
python# Tu implementaciÃ³n
class AnthropicLLM:
    # ... implementaciÃ³n ...

# Aliases para compatibilidad
AzureLLM = AnthropicLLM
Azure = AnthropicLLM
create_azure_llm = create_llm
quick_generate_azure = quick_generate

Actualiza __init__.py para incluir tu proveedor:
pythonfrom .anthropic import (
    Anthropic,
    AnthropicLLM,
    # ... otros exports
)


Esto garantiza que tu proveedor sea un drop-in replacement completo.
ðŸ“š API Reference
Clase Principal (Azure / HuggingLLM)
pythonllm = ProviderLLM(deployment_name, *, config=None, **config_overrides)
ParÃ¡metros:

deployment_name:

Azure: Nombre del deployment (ej: "gpt-4o")
Hugging Face: Nombre del modelo en minÃºsculas (ej: "jonsnow" para JONSNOW_ENDPOINT_URL)


config: Objeto LLMConfig personalizado (opcional)
**config_overrides: Overrides de configuraciÃ³n

MÃ©todo generate()
pythonresponse = llm.generate(
    prompt,
    *,
    variables=None,
    schema=None,
    batch_items=None,
    max_tokens=None,
    temperature=None
)
ParÃ¡metros:

prompt: Texto del prompt (puede contener {variables})
variables: Dict con valores para sustituir en el prompt
schema: Esquema JSON para salida estructurada
batch_items: Lista de diccionarios para procesamiento por lotes
max_tokens: LÃ­mite de tokens
temperature: Nivel de creatividad (0.0-1.0)

Retorna:

String para salida de texto simple
Dict para salida estructurada con schema
List para procesamiento por lotes

MÃ©todo template()
pythontemplate = llm.template(template_string, *, schema=None, **fixed_params)
Crea una plantilla reutilizable con parÃ¡metros fijos.
Funciones de Conveniencia
quick_generate()
python# Azure
from utils.llm import quick_generate
response = quick_generate(prompt, deployment_name="gpt-4o", **kwargs)

# Hugging Face
from utils.llm.hugging import quick_generate
response = quick_generate(prompt, deployment_name="jonsnow", **kwargs)
create_llm()
python# Azure
from utils.llm import create_llm
llm = create_llm(deployment_name="gpt-4o", **config_overrides)

# Hugging Face
from utils.llm.hugging import create_llm
llm = create_llm(deployment_name="jonsnow", **config_overrides)
Schema
Manejo de esquemas JSON optimizados para cada proveedor.
pythonfrom utils.llm import Schema

# Desde dict
schema = Schema.load({
    "type": "object",
    "properties": {"name": {"type": "string"}},
    "required": ["name"]
})

# Desde archivo YAML
schema = Schema.load("mi_schema.yaml")
ðŸ’¡ Tips y Mejores PrÃ¡cticas
ðŸŽ¯ ConfiguraciÃ³n de Modelos Hugging Face
python# Nombre del modelo = clave del endpoint en minÃºsculas
# JONSNOW_ENDPOINT_URL â†’ HuggingLLM("jonsnow")
# MISTRAL_ENDPOINT_URL â†’ HuggingLLM("mistral")
# MI_MODELO_ENDPOINT_URL â†’ HuggingLLM("mi_modelo")

# El token HF_TOKEN es compartido por todos los modelos
ðŸ”’ Esquemas segÃºn Proveedor
python# Azure optimiza automÃ¡ticamente para strict mode
# Hugging Face usa TGI grammar constraints

# El mismo esquema funciona en ambos
schema = {
    "type": "object",
    "properties": {
        "campo": {"type": "string"}
    }
}
ðŸ”„ MigraciÃ³n entre Proveedores
python# Cambiar de proveedor es tan simple como cambiar el import
# De Azure a Hugging Face:
# from utils.llm import Azure as LLM
from utils.llm import HuggingLLM as LLM

# El resto del cÃ³digo permanece idÃ©ntico
llm = LLM("modelo")
response = llm.generate("prompt", schema=schema)
ðŸ§© Variables de Entorno Organizadas
bash# .env organizado por proveedor

# === Azure OpenAI ===
AZURE_OPENAI_ENDPOINT=https://...
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_API_VERSION=2024-02-15-preview

# === Hugging Face ===
HF_TOKEN=hf_...

# Modelos mÃ©dicos
JONSNOW_ENDPOINT_URL=https://...
MEDLLAMA_ENDPOINT_URL=https://...

# Modelos generales
MISTRAL_ENDPOINT_URL=https://...
LLAMA_ENDPOINT_URL=https://...
ðŸš€ Rendimiento por Proveedor
python# Azure: Mejor para aplicaciones en producciÃ³n
# - LÃ­mites de rate mÃ¡s altos
# - SLAs empresariales
# - Menor latencia

# Hugging Face: Ideal para prototipos y modelos especializados
# - Modelos open source
# - PersonalizaciÃ³n completa
# - Endpoints privados
ðŸ”§ Troubleshooting
Error: Missing Endpoint Variables
bash# Azure
ValueError: Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY

# Hugging Face
ValueError: Missing JONSNOW_ENDPOINT_URL or HF_TOKEN
SoluciÃ³n: Verifica tu archivo .env y que los nombres de las variables coincidan con el patrÃ³n esperado.
Error: Modelo No Encontrado (Hugging Face)
python# Si usas HuggingLLM("mimodelo")
# Debe existir MIMODELO_ENDPOINT_URL en .env
SoluciÃ³n: El nombre del modelo en el cÃ³digo debe coincidir (en mayÃºsculas) con el prefijo de _ENDPOINT_URL.
Respuesta No-JSON con Schema
UserWarning: Model returned non-JSON despite schema constraint
SoluciÃ³n: Normal en ambos proveedores. La librerÃ­a maneja automÃ¡ticamente estos casos y devuelve el texto raw.
ðŸ“„ Compatibilidad
Ambos proveedores mantienen compatibilidad total:
python# Estos parÃ¡metros legacy funcionan en ambos
response = llm.generate(
    "Hola {nombre}",
    prompt_vars={"nombre": "Juan"},    # Usar 'variables' en cÃ³digo nuevo
    output_schema=schema               # Usar 'schema' en cÃ³digo nuevo
)
ðŸŽ‰ Ejemplo Completo Multi-Proveedor
pythonfrom utils.llm import Azure, HuggingLLM

# Configurar ambos proveedores
azure_llm = Azure("gpt-4o")
hf_llm = HuggingLLM("jonsnow")

# Mismo esquema para ambos
schema = {
    "type": "object",
    "properties": {
        "diagnostico": {"type": "string"},
        "confianza": {"type": "number"},
        "severidad": {"type": "string"}
    },
    "required": ["diagnostico", "confianza", "severidad"]
}

# Mismo prompt
sintomas = "Paciente con dolor torÃ¡cico, disnea y sudoraciÃ³n"

# Comparar resultados
resultado_azure = azure_llm.generate(
    f"Analiza estos sÃ­ntomas: {sintomas}",
    schema=schema
)

resultado_hf = hf_llm.generate(
    f"Analiza estos sÃ­ntomas: {sintomas}",
    schema=schema
)

print("Azure GPT-4:", resultado_azure)
print("JohnSnow HF:", resultado_hf)


# AzureLLM v4 ðŸš€

**Wrapper elegante y poderoso para Azure OpenAI API**

Una librerÃ­a diseÃ±ada para desarrolladores que quieren mÃ¡xima productividad con mÃ­nima complejidad. ConfiguraciÃ³n automÃ¡tica, esquemas JSON, plantillas reutilizables y cÃ³digo limpio.

## ðŸŽ¯ CaracterÃ­sticas Principales

- âœ… **ConfiguraciÃ³n automÃ¡tica** desde variables de entorno
- âœ… **Salida estructurada** con esquemas JSON
- âœ… **Sistema de plantillas** reutilizables
- âœ… **Procesamiento por lotes (batch)** eficiente
- âœ… **API limpia** con alias intuitivos
- âœ… **Manejo de errores** elegante
- âœ… **Compatibilidad hacia atrÃ¡s** total
- âœ… **Zero-config** para casos simples

## ðŸ“¦ InstalaciÃ³n

```bash
pip install openai python-dotenv pyyaml
```

> **Note:** La librerÃ­a requiere `openai` para Azure OpenAI y opcionalmente `python-dotenv` para variables de entorno.

## âš™ï¸ ConfiguraciÃ³n

### Variables de Entorno

Crea un archivo `.env` en tu proyecto:

```bash
AZURE_OPENAI_ENDPOINT=https://tu-recurso.openai.azure.com/
AZURE_OPENAI_API_KEY=tu-api-key-aqui
AZURE_OPENAI_API_VERSION=2024-02-15-preview
```

> **Tip:** La librerÃ­a carga automÃ¡ticamente las variables de entorno. No necesitas configuraciÃ³n adicional.

## ðŸš€ Inicio RÃ¡pido

### Uso BÃ¡sico

```python
from utils.llm import Azure

# Â¡ConfiguraciÃ³n automÃ¡tica desde .env!
llm = Azure("gpt-4o")
response = llm.generate("Explica quÃ© es la inteligencia artificial")
print(response)
```

### Una LÃ­nea de CÃ³digo

```python
from utils.llm import quick_generate

# Para casos sÃºper simples
response = quick_generate("Traduce 'Hello' al espaÃ±ol")
```

## ðŸ“ Ejemplos de Uso

### 1. GeneraciÃ³n BÃ¡sica con ParÃ¡metros

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

# Con temperatura baja para respuestas consistentes
response = llm.generate(
    "Resume este texto en 3 puntos clave",
    temperature=0.2,
    max_tokens=150
)
```

### 2. Salida Estructurada con JSON

```python
# Definir esquema de salida
schema = {
    "type": "object",
    "properties": {
        "resumen": {"type": "string"},
        "puntos_clave": {
            "type": "array",
            "items": {"type": "string"}
        },
        "categoria": {"type": "string"}
    },
    "required": ["resumen", "puntos_clave", "categoria"]
}

# Generar respuesta estructurada
response = llm.generate(
    "Analiza este artÃ­culo sobre IA...",
    schema=schema
)

# response es un dict con la estructura definida
print(response["resumen"])
print(response["puntos_clave"])
```

### 3. Plantillas Reutilizables

```python
# Crear plantilla con variables
traductor = llm.template(
    "Traduce '{texto}' del {idioma_origen} al {idioma_destino}",
    temperature=0.3
)

# Usar la plantilla mÃºltiples veces
resultado1 = traductor(
    texto="Hello world", 
    idioma_origen="inglÃ©s", 
    idioma_destino="espaÃ±ol"
)

resultado2 = traductor(
    texto="Bonjour", 
    idioma_origen="francÃ©s", 
    idioma_destino="espaÃ±ol"
)
```

### 4. Plantillas con Esquemas

```python
# Plantilla que siempre devuelve JSON estructurado
analizador = llm.template(
    "Analiza el sentimiento de: '{texto}'",
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {"type": "string"},
            "confianza": {"type": "number"},
            "palabras_clave": {"type": "array", "items": {"type": "string"}}
        },
        "required": ["sentimiento", "confianza", "palabras_clave"]
    }
)

resultado = analizador(texto="Â¡Me encanta este producto!")
print(f"Sentimiento: {resultado['sentimiento']}")
print(f"Confianza: {resultado['confianza']}")
```

### 5. Procesamiento por Lotes (Batch)

```python
# Procesar mÃºltiples items en una sola llamada
items = [
    {"id": 1, "text": "Hello world"},
    {"id": 2, "text": "Good morning"},
    {"id": 3, "text": "How are you?"}
]

# Sin schema - devuelve lista de strings
responses = llm.generate(
    "Traduce cada texto al espaÃ±ol",
    batch_items=items
)
# responses = ["Hola mundo", "Buenos dÃ­as", "Â¿CÃ³mo estÃ¡s?"]

# Con schema - devuelve lista estructurada
schema = {
    "type": "object",
    "properties": {
        "original": {"type": "string"},
        "traduccion": {"type": "string"},
        "idioma": {"type": "string"}
    }
}

responses = llm.generate(
    "Traduce cada texto al espaÃ±ol y detecta el idioma original",
    batch_items=items,
    schema=schema
)
# responses = [
#     {"original": "Hello world", "traduccion": "Hola mundo", "idioma": "inglÃ©s"},
#     {"original": "Good morning", "traduccion": "Buenos dÃ­as", "idioma": "inglÃ©s"},
#     {"original": "How are you?", "traduccion": "Â¿CÃ³mo estÃ¡s?", "idioma": "inglÃ©s"}
# ]
```

## ðŸ”§ ConfiguraciÃ³n Avanzada

### ConfiguraciÃ³n Personalizada

```python
from utils.llm import LLMConfig, AzureLLM

# ConfiguraciÃ³n explÃ­cita
config = LLMConfig(
    endpoint="https://mi-recurso.openai.azure.com/",
    api_key="mi-api-key",
    deployment_name="gpt-4o",
    temperature=0.7,
    validate_schema=True  # ValidaciÃ³n strict de esquemas
)

llm = AzureLLM(config=config)
```

### ConfiguraciÃ³n desde Entorno con Overrides

```python
from utils.llm import Azure

# Usar variables de entorno pero override algunos valores
llm = Azure("gpt-4o", temperature=0.1, validate_schema=True)
```

## ðŸ“š API Reference

### Azure / AzureLLM

La clase principal para interactuar con Azure OpenAI.

```python
llm = Azure(deployment_name, *, config=None, **config_overrides)
```

**ParÃ¡metros:**
- `deployment_name`: Nombre del deployment (ej: "gpt-4o")
- `config`: Objeto LLMConfig personalizado (opcional)
- `**config_overrides`: Overrides de configuraciÃ³n

#### MÃ©todo `generate()`

```python
response = llm.generate(
    prompt,
    *,
    variables=None,
    schema=None,
    batch_items=None,
    max_tokens=None,
    temperature=None
)
```

**ParÃ¡metros:**
- `prompt`: Texto del prompt (puede contener {variables})
- `variables`: Dict con valores para sustituir en el prompt
- `schema`: Esquema JSON para salida estructurada
- `batch_items`: Lista de diccionarios para procesamiento por lotes
- `max_tokens`: LÃ­mite de tokens
- `temperature`: Nivel de creatividad (0.0-1.0)

**Retorna:**
- String para salida de texto simple
- Dict para salida estructurada con schema
- List para procesamiento por lotes

#### MÃ©todo `template()`

```python
template = llm.template(template_string, *, schema=None, **fixed_params)
```

Crea una plantilla reutilizable con parÃ¡metros fijos.

### Funciones de Conveniencia

#### `quick_generate()`

```python
from utils.llm import quick_generate

response = quick_generate(prompt, deployment_name="gpt-4o", **kwargs)
```

GeneraciÃ³n rÃ¡pida en una lÃ­nea para casos simples.

#### `create_llm()`

```python
from utils.llm import create_llm

llm = create_llm(deployment_name="gpt-4o", **config_overrides)
```

Factory function para crear instancias de LLM.

### Schema

Manejo de esquemas JSON optimizados para Azure OpenAI.

```python
from utils.llm import Schema

# Desde dict
schema = Schema.load({
    "type": "object",
    "properties": {"name": {"type": "string"}},
    "required": ["name"]
})

# Desde archivo YAML
schema = Schema.load("mi_schema.yaml")
```

## ðŸ’¡ Tips y Mejores PrÃ¡cticas

### ðŸŽ¯ OptimizaciÃ³n de Costos

```python
# Usa max_tokens para controlar costos
response = llm.generate(
    "Resume en 50 palabras...", 
    max_tokens=100  # Limita la respuesta
)

# Temperatura baja para respuestas mÃ¡s predecibles
llm = Azure("gpt-4o", temperature=0.1)
```

### ðŸ”’ Esquemas Robustos

```python
# Los esquemas se optimizan automÃ¡ticamente para Azure
schema = {
    "type": "object",
    "properties": {
        "campo": {"type": "string"}
    }
    # Azure aÃ±ade automÃ¡ticamente:
    # "additionalProperties": false
    # "required": ["campo"]
}
```

> **Note:** AzureLLM optimiza automÃ¡ticamente los esquemas para el modo strict de Azure OpenAI.

### ðŸ”„ ReutilizaciÃ³n de Instancias

```python
# âœ… Buena prÃ¡ctica: reutilizar instancia
llm = Azure("gpt-4o")
for texto in textos:
    response = llm.generate(f"Analiza: {texto}")

# âŒ Evitar: crear nueva instancia cada vez
for texto in textos:
    llm = Azure("gpt-4o")  # Ineficiente
    response = llm.generate(f"Analiza: {texto}")
```

### ðŸ§© Variables en Plantillas

```python
# âœ… Usa variables para prompts dinÃ¡micos
template = llm.template("Traduce '{texto}' al {idioma}")
result = template(texto="Hello", idioma="espaÃ±ol")

# âŒ Evitar concatenaciÃ³n manual
prompt = f"Traduce '{texto}' al {idioma}"  # Menos flexible
```

### ðŸš€ Procesamiento Eficiente por Lotes

```python
# âœ… Procesar mÃºltiples items en una sola llamada
items = [{"name": "Alice"}, {"name": "Bob"}, {"name": "Charlie"}]
greetings = llm.generate(
    "Genera un saludo personalizado para cada persona",
    batch_items=items
)

# âŒ Evitar: mÃºltiples llamadas individuales
for item in items:
    greeting = llm.generate(f"Saluda a {item['name']}")  # Ineficiente
```

> **Tip:** El procesamiento por lotes es ideal para transformaciones cortas y consistentes. Para outputs muy largos, usa lotes mÃ¡s pequeÃ±os para mantener la calidad.

### Procesador de Datos por Lotes

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

# Datos a procesar
productos = [
    {"nombre": "Laptop", "precio": 999},
    {"nombre": "Mouse", "precio": 29},
    {"nombre": "Teclado", "precio": 79}
]

# Schema para la respuesta
schema = {
    "type": "object",
    "properties": {
        "nombre": {"type": "string"},
        "categoria": {"type": "string"},
        "rango_precio": {"type": "string"},
        "descripcion_marketing": {"type": "string"}
    }
}

# Procesar todos los productos en una sola llamada
resultados = llm.generate(
    """Para cada producto, determina su categorÃ­a, rango de precio 
    (econÃ³mico/medio/premium) y genera una descripciÃ³n de marketing atractiva""",
    batch_items=productos,
    schema=schema,
    temperature=0.7
)

# Mostrar resultados
for i, resultado in enumerate(resultados):
    print(f"\nProducto {i+1}:")
    print(f"  Nombre: {resultado['nombre']}")
    print(f"  CategorÃ­a: {resultado['categoria']}")
    print(f"  Rango: {resultado['rango_precio']}")
    print(f"  Marketing: {resultado['descripcion_marketing']}")
```

## ðŸ”§ Troubleshooting

### Error: Missing AZURE_OPENAI_ENDPOINT

```bash
ValueError: Missing AZURE_OPENAI_ENDPOINT or AZURE_OPENAI_API_KEY
```

**SoluciÃ³n:** Verifica que tienes un archivo `.env` con las variables correctas o pasa la configuraciÃ³n explÃ­citamente.

### Error: KeyError en Template

```python
KeyError: Missing template variable: 'nombre'
```

**SoluciÃ³n:** AsegÃºrate de pasar todas las variables definidas en el template:

```python
template = llm.template("Hola {nombre}")
response = template(nombre="Juan")  # âœ… Variable proporcionada
```

### Respuesta No-JSON con Schema

```
UserWarning: Model returned non-JSON despite schema constraint
```

**SoluciÃ³n:** Esto es normal. AzureLLM maneja automÃ¡ticamente casos donde el modelo no respeta el esquema y devuelve el texto raw.

### ValidaciÃ³n de Schema

```python
# Habilitar validaciÃ³n strict (requiere jsonschema)
pip install jsonschema

llm = Azure("gpt-4o", validate_schema=True)
```

## ðŸ“„ Compatibilidad hacia AtrÃ¡s

AzureLLM v4 mantiene compatibilidad total con versiones anteriores:

```python
# ParÃ¡metros antiguos aÃºn funcionan
response = llm.generate(
    "Hola {nombre}",
    prompt_vars={"nombre": "Juan"},    # Ahora se llama 'variables'
    output_schema=schema               # Ahora se llama 'schema'
)
```

> **Note:** Se recomienda usar los nuevos nombres de parÃ¡metros para cÃ³digo nuevo.

## ðŸŽ‰ Ejemplos Completos

### Analizador de Sentimientos

```python
from utils.llm import Azure

llm = Azure("gpt-4o")

analizador = llm.template(
    """Analiza el sentimiento del siguiente texto: "{texto}"
    
    Responde con el sentimiento (positivo/negativo/neutro) y una puntuaciÃ³n de confianza del 0 al 1.""",
    
    schema={
        "type": "object",
        "properties": {
            "sentimiento": {
                "type": "string",
                "enum": ["positivo", "negativo", "neutro"]
            },
            "confianza": {
                "type": "number",
                "minimum": 0,
                "maximum": 1
            },
            "razon": {"type": "string"}
        },
        "required": ["sentimiento", "confianza", "razon"]
    },
    temperature=0.2
)

# Usar el analizador
textos = [
    "Â¡Me encanta este producto!",
    "El servicio fue terrible",
    "El clima estÃ¡ nublado hoy"
]

for texto in textos:
    resultado = analizador(texto=texto)
    print(f"Texto: {texto}")
    print(f"Sentimiento: {resultado['sentimiento']} ({resultado['confianza']:.2f})")
    print(f"RazÃ³n: {resultado['razon']}\n")
```